// Generated by the protocol buffer compiler.  DO NOT EDIT!
// NO CHECKED-IN PROTOBUF GENCODE
// source: tensorflow/core/protobuf/config.proto
// Protobuf Java Version: 4.28.3

package org.tensorflow.proto;

/**
 * Protobuf type {@code tensorflow.GPUOptions}
 */
public final class GPUOptions extends
    com.google.protobuf.GeneratedMessage implements
    // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions)
    GPUOptionsOrBuilder {
private static final long serialVersionUID = 0L;
  static {
    com.google.protobuf.RuntimeVersion.validateProtobufGencodeVersion(
      com.google.protobuf.RuntimeVersion.RuntimeDomain.PUBLIC,
      /* major= */ 4,
      /* minor= */ 28,
      /* patch= */ 3,
      /* suffix= */ "",
      GPUOptions.class.getName());
  }
  // Use GPUOptions.newBuilder() to construct.
  private GPUOptions(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
    super(builder);
  }
  private GPUOptions() {
    allocatorType_ = "";
    visibleDeviceList_ = "";
  }

  public static final com.google.protobuf.Descriptors.Descriptor
      getDescriptor() {
    return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
  }

  @java.lang.Override
  protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internalGetFieldAccessorTable() {
    return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            org.tensorflow.proto.GPUOptions.class, org.tensorflow.proto.GPUOptions.Builder.class);
  }

  public interface ExperimentalOrBuilder extends
      // @@protoc_insertion_point(interface_extends:tensorflow.GPUOptions.Experimental)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> 
        getVirtualDevicesList();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index);
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    int getVirtualDevicesCount();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    java.util.List<? extends org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
        getVirtualDevicesOrBuilderList();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
        int index);

    /**
     * <pre>
     * The number of virtual devices to create on each visible GPU. The
     * available memory will be split equally among all virtual devices. If the
     * field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
     * be ignored.
     * </pre>
     *
     * <code>int32 num_virtual_devices_per_gpu = 15;</code>
     * @return The numVirtualDevicesPerGpu.
     */
    int getNumVirtualDevicesPerGpu();

    /**
     * <pre>
     * If true, uses CUDA unified memory for memory allocations. If
     * per_process_gpu_memory_fraction option is greater than 1.0, then unified
     * memory is used regardless of the value for this field. See comments for
     * per_process_gpu_memory_fraction field for more details and requirements
     * of the unified memory. This option is useful to oversubscribe memory if
     * multiple processes are sharing a single GPU while individually using less
     * than 1.0 per process memory fraction.
     * </pre>
     *
     * <code>bool use_unified_memory = 2;</code>
     * @return The useUnifiedMemory.
     */
    boolean getUseUnifiedMemory();

    /**
     * <pre>
     * If &gt; 1, the number of device-to-device copy streams to create
     * for each GPUDevice.  Default value is 0, which is automatically
     * converted to 1.
     * </pre>
     *
     * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
     * @return The numDevToDevCopyStreams.
     */
    int getNumDevToDevCopyStreams();

    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     * @return The collectiveRingOrder.
     */
    java.lang.String getCollectiveRingOrder();
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     * @return The bytes for collectiveRingOrder.
     */
    com.google.protobuf.ByteString
        getCollectiveRingOrderBytes();

    /**
     * <pre>
     * If true then extra work is done by GPUDevice and GPUBFCAllocator to
     * keep track of when GPU memory is freed and when kernels actually
     * complete so that we can know when a nominally free memory chunk
     * is really not subject to pending use.
     * </pre>
     *
     * <code>bool timestamped_allocator = 5;</code>
     * @return The timestampedAllocator.
     */
    boolean getTimestampedAllocator();

    /**
     * <pre>
     * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
     * Note that timestamped_allocator is only effective if some tracking is
     * specified.
     *
     * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
     * is inserted after every n kernels without an event.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_interval = 7;</code>
     * @return The kernelTrackerMaxInterval.
     */
    int getKernelTrackerMaxInterval();

    /**
     * <pre>
     * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
     * inserted after every series of kernels allocating a sum of
     * memory &gt;= n.  If one kernel allocates b * n bytes, then one
     * event will be inserted after it, but it will count as b against
     * the pending limit.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_bytes = 8;</code>
     * @return The kernelTrackerMaxBytes.
     */
    int getKernelTrackerMaxBytes();

    /**
     * <pre>
     * If kernel_tracker_max_pending &gt; 0 then no more than this many
     * tracking events can be outstanding at a time.  An attempt to
     * launch an additional kernel will stall until an event
     * completes.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_pending = 9;</code>
     * @return The kernelTrackerMaxPending.
     */
    int getKernelTrackerMaxPending();

    /**
     * <pre>
     * BFC Allocator can return an allocated chunk of memory upto 2x the
     * requested size. For virtual devices with tight memory constraints, and
     * proportionately large allocation requests, this can lead to a significant
     * reduction in available memory. The threshold below controls when a chunk
     * should be split if the chunk size exceeds requested memory size. It is
     * expressed as a fraction of total available memory for the tf device. For
     * example setting it to 0.05 would imply a chunk needs to be split if its
     * size exceeds the requested memory by 5% of the total virtual device/gpu
     * memory size.
     * </pre>
     *
     * <code>double internal_fragmentation_fraction = 10;</code>
     * @return The internalFragmentationFraction.
     */
    double getInternalFragmentationFraction();

    /**
     * <pre>
     * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
     * </pre>
     *
     * <code>bool use_cuda_malloc_async = 11;</code>
     * @return The useCudaMallocAsync.
     */
    boolean getUseCudaMallocAsync();

    /**
     * <pre>
     * By default, BFCAllocator may sleep when it runs out of memory, in the
     * hopes that another thread will free up memory in the meantime.  Setting
     * this to true disables the sleep; instead we'll OOM immediately.
     * </pre>
     *
     * <code>bool disallow_retry_on_allocation_failure = 12;</code>
     * @return The disallowRetryOnAllocationFailure.
     */
    boolean getDisallowRetryOnAllocationFailure();

    /**
     * <pre>
     * Memory limit for "GPU host allocator", aka pinned memory allocator.  This
     * can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
     * </pre>
     *
     * <code>float gpu_host_mem_limit_in_mb = 13;</code>
     * @return The gpuHostMemLimitInMb.
     */
    float getGpuHostMemLimitInMb();

    /**
     * <pre>
     * If true, then the host allocator allocates its max memory all upfront and
     * never grows.  This can be useful for latency-sensitive systems, because
     * growing the GPU host memory pool can be expensive.
     *
     * You probably only want to use this in combination with
     * gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
     * quite high.
     * </pre>
     *
     * <code>bool gpu_host_mem_disallow_growth = 14;</code>
     * @return The gpuHostMemDisallowGrowth.
     */
    boolean getGpuHostMemDisallowGrowth();

    /**
     * <pre>
     * Memory limit for gpu system. This can also be set by
     * TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
     * gpu_system_memory_size_in_mb. With this, user can configure the gpu
     * system memory size for better resource estimation of multi-tenancy(one
     * gpu with multiple model) use case.
     * </pre>
     *
     * <code>int32 gpu_system_memory_size_in_mb = 16;</code>
     * @return The gpuSystemMemorySizeInMb.
     */
    int getGpuSystemMemorySizeInMb();

    /**
     * <pre>
     * If true, save information needed for created a PjRt GPU client for
     * creating a client with remote devices.
     * </pre>
     *
     * <code>bool populate_pjrt_gpu_client_creation_info = 17;</code>
     * @return The populatePjrtGpuClientCreationInfo.
     */
    boolean getPopulatePjrtGpuClientCreationInfo();

    /**
     * <pre>
     * node_id for use when creating a PjRt GPU client with remote devices,
     * which enumerates jobs*tasks from a ServerDef.
     * </pre>
     *
     * <code>int32 node_id = 18;</code>
     * @return The nodeId.
     */
    int getNodeId();

    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     * @return Whether the streamMergeOptions field is set.
     */
    boolean hasStreamMergeOptions();
    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     * @return The streamMergeOptions.
     */
    org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getStreamMergeOptions();
    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     */
    org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder getStreamMergeOptionsOrBuilder();
  }
  /**
   * Protobuf type {@code tensorflow.GPUOptions.Experimental}
   */
  public static final class Experimental extends
      com.google.protobuf.GeneratedMessage implements
      // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions.Experimental)
      ExperimentalOrBuilder {
  private static final long serialVersionUID = 0L;
    static {
      com.google.protobuf.RuntimeVersion.validateProtobufGencodeVersion(
        com.google.protobuf.RuntimeVersion.RuntimeDomain.PUBLIC,
        /* major= */ 4,
        /* minor= */ 28,
        /* patch= */ 3,
        /* suffix= */ "",
        Experimental.class.getName());
    }
    // Use Experimental.newBuilder() to construct.
    private Experimental(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
    }
    private Experimental() {
      virtualDevices_ = java.util.Collections.emptyList();
      collectiveRingOrder_ = "";
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.GPUOptions.Experimental.class, org.tensorflow.proto.GPUOptions.Experimental.Builder.class);
    }

    public interface VirtualDevicesOrBuilder extends
        // @@protoc_insertion_point(interface_extends:tensorflow.GPUOptions.Experimental.VirtualDevices)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @return A list containing the memoryLimitMb.
       */
      java.util.List<java.lang.Float> getMemoryLimitMbList();
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @return The count of memoryLimitMb.
       */
      int getMemoryLimitMbCount();
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @param index The index of the element to return.
       * @return The memoryLimitMb at the given index.
       */
      float getMemoryLimitMb(int index);

      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @return A list containing the priority.
       */
      java.util.List<java.lang.Integer> getPriorityList();
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @return The count of priority.
       */
      int getPriorityCount();
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @param index The index of the element to return.
       * @return The priority at the given index.
       */
      int getPriority(int index);

      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @return A list containing the deviceOrdinal.
       */
      java.util.List<java.lang.Integer> getDeviceOrdinalList();
      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @return The count of deviceOrdinal.
       */
      int getDeviceOrdinalCount();
      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @param index The index of the element to return.
       * @return The deviceOrdinal at the given index.
       */
      int getDeviceOrdinal(int index);
    }
    /**
     * <pre>
     * Configuration for breaking down a visible GPU into multiple "virtual"
     * devices.
     * </pre>
     *
     * Protobuf type {@code tensorflow.GPUOptions.Experimental.VirtualDevices}
     */
    public static final class VirtualDevices extends
        com.google.protobuf.GeneratedMessage implements
        // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions.Experimental.VirtualDevices)
        VirtualDevicesOrBuilder {
    private static final long serialVersionUID = 0L;
      static {
        com.google.protobuf.RuntimeVersion.validateProtobufGencodeVersion(
          com.google.protobuf.RuntimeVersion.RuntimeDomain.PUBLIC,
          /* major= */ 4,
          /* minor= */ 28,
          /* patch= */ 3,
          /* suffix= */ "",
          VirtualDevices.class.getName());
      }
      // Use VirtualDevices.newBuilder() to construct.
      private VirtualDevices(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
      }
      private VirtualDevices() {
        memoryLimitMb_ = emptyFloatList();
        priority_ = emptyIntList();
        deviceOrdinal_ = emptyIntList();
      }

      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.class, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder.class);
      }

      public static final int MEMORY_LIMIT_MB_FIELD_NUMBER = 1;
      @SuppressWarnings("serial")
      private com.google.protobuf.Internal.FloatList memoryLimitMb_ =
          emptyFloatList();
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @return A list containing the memoryLimitMb.
       */
      @java.lang.Override
      public java.util.List<java.lang.Float>
          getMemoryLimitMbList() {
        return memoryLimitMb_;
      }
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @return The count of memoryLimitMb.
       */
      public int getMemoryLimitMbCount() {
        return memoryLimitMb_.size();
      }
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty and `num_virtual_devices_per_gpu` is not set, it will create
       * single virtual device taking all available memory from the device.
       *
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       * @param index The index of the element to return.
       * @return The memoryLimitMb at the given index.
       */
      public float getMemoryLimitMb(int index) {
        return memoryLimitMb_.getFloat(index);
      }
      private int memoryLimitMbMemoizedSerializedSize = -1;

      public static final int PRIORITY_FIELD_NUMBER = 2;
      @SuppressWarnings("serial")
      private com.google.protobuf.Internal.IntList priority_ =
          emptyIntList();
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @return A list containing the priority.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getPriorityList() {
        return priority_;
      }
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @return The count of priority.
       */
      public int getPriorityCount() {
        return priority_.size();
      }
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       *
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       *
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       * @param index The index of the element to return.
       * @return The priority at the given index.
       */
      public int getPriority(int index) {
        return priority_.getInt(index);
      }
      private int priorityMemoizedSerializedSize = -1;

      public static final int DEVICE_ORDINAL_FIELD_NUMBER = 3;
      @SuppressWarnings("serial")
      private com.google.protobuf.Internal.IntList deviceOrdinal_ =
          emptyIntList();
      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @return A list containing the deviceOrdinal.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getDeviceOrdinalList() {
        return deviceOrdinal_;
      }
      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @return The count of deviceOrdinal.
       */
      public int getDeviceOrdinalCount() {
        return deviceOrdinal_.size();
      }
      /**
       * <pre>
       * Virtual Device ordinal number determines the device ID of the device.
       * A Virtual device with a lower ordinal number always receives the a
       * smaller device id. The phyiscal device id and location in the
       * virtual device list is used to break ties.
       * </pre>
       *
       * <code>repeated int32 device_ordinal = 3;</code>
       * @param index The index of the element to return.
       * @return The deviceOrdinal at the given index.
       */
      public int getDeviceOrdinal(int index) {
        return deviceOrdinal_.getInt(index);
      }
      private int deviceOrdinalMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getMemoryLimitMbList().size() > 0) {
          output.writeUInt32NoTag(10);
          output.writeUInt32NoTag(memoryLimitMbMemoizedSerializedSize);
        }
        for (int i = 0; i < memoryLimitMb_.size(); i++) {
          output.writeFloatNoTag(memoryLimitMb_.getFloat(i));
        }
        if (getPriorityList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(priorityMemoizedSerializedSize);
        }
        for (int i = 0; i < priority_.size(); i++) {
          output.writeInt32NoTag(priority_.getInt(i));
        }
        if (getDeviceOrdinalList().size() > 0) {
          output.writeUInt32NoTag(26);
          output.writeUInt32NoTag(deviceOrdinalMemoizedSerializedSize);
        }
        for (int i = 0; i < deviceOrdinal_.size(); i++) {
          output.writeInt32NoTag(deviceOrdinal_.getInt(i));
        }
        getUnknownFields().writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          dataSize = 4 * getMemoryLimitMbList().size();
          size += dataSize;
          if (!getMemoryLimitMbList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          memoryLimitMbMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          for (int i = 0; i < priority_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(priority_.getInt(i));
          }
          size += dataSize;
          if (!getPriorityList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          priorityMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          for (int i = 0; i < deviceOrdinal_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(deviceOrdinal_.getInt(i));
          }
          size += dataSize;
          if (!getDeviceOrdinalList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          deviceOrdinalMemoizedSerializedSize = dataSize;
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices)) {
          return super.equals(obj);
        }
        org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices other = (org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices) obj;

        if (!getMemoryLimitMbList()
            .equals(other.getMemoryLimitMbList())) return false;
        if (!getPriorityList()
            .equals(other.getPriorityList())) return false;
        if (!getDeviceOrdinalList()
            .equals(other.getDeviceOrdinalList())) return false;
        if (!getUnknownFields().equals(other.getUnknownFields())) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getMemoryLimitMbCount() > 0) {
          hash = (37 * hash) + MEMORY_LIMIT_MB_FIELD_NUMBER;
          hash = (53 * hash) + getMemoryLimitMbList().hashCode();
        }
        if (getPriorityCount() > 0) {
          hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
          hash = (53 * hash) + getPriorityList().hashCode();
        }
        if (getDeviceOrdinalCount() > 0) {
          hash = (37 * hash) + DEVICE_ORDINAL_FIELD_NUMBER;
          hash = (53 * hash) + getDeviceOrdinalList().hashCode();
        }
        hash = (29 * hash) + getUnknownFields().hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseDelimitedWithIOException(PARSER, input);
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       * Configuration for breaking down a visible GPU into multiple "virtual"
       * devices.
       * </pre>
       *
       * Protobuf type {@code tensorflow.GPUOptions.Experimental.VirtualDevices}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions.Experimental.VirtualDevices)
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.class, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder.class);
        }

        // Construct using org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.newBuilder()
        private Builder() {

        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);

        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          bitField0_ = 0;
          memoryLimitMb_ = emptyFloatList();
          priority_ = emptyIntList();
          deviceOrdinal_ = emptyIntList();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getDefaultInstanceForType() {
          return org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.getDefaultInstance();
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices build() {
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices buildPartial() {
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices result = new org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices(this);
          if (bitField0_ != 0) { buildPartial0(result); }
          onBuilt();
          return result;
        }

        private void buildPartial0(org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices result) {
          int from_bitField0_ = bitField0_;
          if (((from_bitField0_ & 0x00000001) != 0)) {
            memoryLimitMb_.makeImmutable();
            result.memoryLimitMb_ = memoryLimitMb_;
          }
          if (((from_bitField0_ & 0x00000002) != 0)) {
            priority_.makeImmutable();
            result.priority_ = priority_;
          }
          if (((from_bitField0_ & 0x00000004) != 0)) {
            deviceOrdinal_.makeImmutable();
            result.deviceOrdinal_ = deviceOrdinal_;
          }
        }

        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices) {
            return mergeFrom((org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices other) {
          if (other == org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.getDefaultInstance()) return this;
          if (!other.memoryLimitMb_.isEmpty()) {
            if (memoryLimitMb_.isEmpty()) {
              memoryLimitMb_ = other.memoryLimitMb_;
              memoryLimitMb_.makeImmutable();
              bitField0_ |= 0x00000001;
            } else {
              ensureMemoryLimitMbIsMutable();
              memoryLimitMb_.addAll(other.memoryLimitMb_);
            }
            onChanged();
          }
          if (!other.priority_.isEmpty()) {
            if (priority_.isEmpty()) {
              priority_ = other.priority_;
              priority_.makeImmutable();
              bitField0_ |= 0x00000002;
            } else {
              ensurePriorityIsMutable();
              priority_.addAll(other.priority_);
            }
            onChanged();
          }
          if (!other.deviceOrdinal_.isEmpty()) {
            if (deviceOrdinal_.isEmpty()) {
              deviceOrdinal_ = other.deviceOrdinal_;
              deviceOrdinal_.makeImmutable();
              bitField0_ |= 0x00000004;
            } else {
              ensureDeviceOrdinalIsMutable();
              deviceOrdinal_.addAll(other.deviceOrdinal_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 13: {
                  float v = input.readFloat();
                  ensureMemoryLimitMbIsMutable();
                  memoryLimitMb_.addFloat(v);
                  break;
                } // case 13
                case 10: {
                  int length = input.readRawVarint32();
                  int limit = input.pushLimit(length);
                  int alloc = length > 4096 ? 4096 : length;
                  ensureMemoryLimitMbIsMutable(alloc / 4);
                  while (input.getBytesUntilLimit() > 0) {
                    memoryLimitMb_.addFloat(input.readFloat());
                  }
                  input.popLimit(limit);
                  break;
                } // case 10
                case 16: {
                  int v = input.readInt32();
                  ensurePriorityIsMutable();
                  priority_.addInt(v);
                  break;
                } // case 16
                case 18: {
                  int length = input.readRawVarint32();
                  int limit = input.pushLimit(length);
                  ensurePriorityIsMutable();
                  while (input.getBytesUntilLimit() > 0) {
                    priority_.addInt(input.readInt32());
                  }
                  input.popLimit(limit);
                  break;
                } // case 18
                case 24: {
                  int v = input.readInt32();
                  ensureDeviceOrdinalIsMutable();
                  deviceOrdinal_.addInt(v);
                  break;
                } // case 24
                case 26: {
                  int length = input.readRawVarint32();
                  int limit = input.pushLimit(length);
                  ensureDeviceOrdinalIsMutable();
                  while (input.getBytesUntilLimit() > 0) {
                    deviceOrdinal_.addInt(input.readInt32());
                  }
                  input.popLimit(limit);
                  break;
                } // case 26
                default: {
                  if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                    done = true; // was an endgroup tag
                  }
                  break;
                } // default:
              } // switch (tag)
            } // while (!done)
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.unwrapIOException();
          } finally {
            onChanged();
          } // finally
          return this;
        }
        private int bitField0_;

        private com.google.protobuf.Internal.FloatList memoryLimitMb_ = emptyFloatList();
        private void ensureMemoryLimitMbIsMutable() {
          if (!memoryLimitMb_.isModifiable()) {
            memoryLimitMb_ = makeMutableCopy(memoryLimitMb_);
          }
          bitField0_ |= 0x00000001;
        }
        private void ensureMemoryLimitMbIsMutable(int capacity) {
          if (!memoryLimitMb_.isModifiable()) {
            memoryLimitMb_ = makeMutableCopy(memoryLimitMb_, capacity);
          }
          bitField0_ |= 0x00000001;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @return A list containing the memoryLimitMb.
         */
        public java.util.List<java.lang.Float>
            getMemoryLimitMbList() {
          memoryLimitMb_.makeImmutable();
          return memoryLimitMb_;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @return The count of memoryLimitMb.
         */
        public int getMemoryLimitMbCount() {
          return memoryLimitMb_.size();
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @param index The index of the element to return.
         * @return The memoryLimitMb at the given index.
         */
        public float getMemoryLimitMb(int index) {
          return memoryLimitMb_.getFloat(index);
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @param index The index to set the value at.
         * @param value The memoryLimitMb to set.
         * @return This builder for chaining.
         */
        public Builder setMemoryLimitMb(
            int index, float value) {

          ensureMemoryLimitMbIsMutable();
          memoryLimitMb_.setFloat(index, value);
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @param value The memoryLimitMb to add.
         * @return This builder for chaining.
         */
        public Builder addMemoryLimitMb(float value) {

          ensureMemoryLimitMbIsMutable();
          memoryLimitMb_.addFloat(value);
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @param values The memoryLimitMb to add.
         * @return This builder for chaining.
         */
        public Builder addAllMemoryLimitMb(
            java.lang.Iterable<? extends java.lang.Float> values) {
          ensureMemoryLimitMbIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, memoryLimitMb_);
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty and `num_virtual_devices_per_gpu` is not set, it will create
         * single virtual device taking all available memory from the device.
         *
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMemoryLimitMb() {
          memoryLimitMb_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList priority_ = emptyIntList();
        private void ensurePriorityIsMutable() {
          if (!priority_.isModifiable()) {
            priority_ = makeMutableCopy(priority_);
          }
          bitField0_ |= 0x00000002;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @return A list containing the priority.
         */
        public java.util.List<java.lang.Integer>
            getPriorityList() {
          priority_.makeImmutable();
          return priority_;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @return The count of priority.
         */
        public int getPriorityCount() {
          return priority_.size();
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @param index The index of the element to return.
         * @return The priority at the given index.
         */
        public int getPriority(int index) {
          return priority_.getInt(index);
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @param index The index to set the value at.
         * @param value The priority to set.
         * @return This builder for chaining.
         */
        public Builder setPriority(
            int index, int value) {

          ensurePriorityIsMutable();
          priority_.setInt(index, value);
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @param value The priority to add.
         * @return This builder for chaining.
         */
        public Builder addPriority(int value) {

          ensurePriorityIsMutable();
          priority_.addInt(value);
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @param values The priority to add.
         * @return This builder for chaining.
         */
        public Builder addAllPriority(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensurePriorityIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, priority_);
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         *
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         *
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearPriority() {
          priority_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList deviceOrdinal_ = emptyIntList();
        private void ensureDeviceOrdinalIsMutable() {
          if (!deviceOrdinal_.isModifiable()) {
            deviceOrdinal_ = makeMutableCopy(deviceOrdinal_);
          }
          bitField0_ |= 0x00000004;
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @return A list containing the deviceOrdinal.
         */
        public java.util.List<java.lang.Integer>
            getDeviceOrdinalList() {
          deviceOrdinal_.makeImmutable();
          return deviceOrdinal_;
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @return The count of deviceOrdinal.
         */
        public int getDeviceOrdinalCount() {
          return deviceOrdinal_.size();
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @param index The index of the element to return.
         * @return The deviceOrdinal at the given index.
         */
        public int getDeviceOrdinal(int index) {
          return deviceOrdinal_.getInt(index);
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @param index The index to set the value at.
         * @param value The deviceOrdinal to set.
         * @return This builder for chaining.
         */
        public Builder setDeviceOrdinal(
            int index, int value) {

          ensureDeviceOrdinalIsMutable();
          deviceOrdinal_.setInt(index, value);
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @param value The deviceOrdinal to add.
         * @return This builder for chaining.
         */
        public Builder addDeviceOrdinal(int value) {

          ensureDeviceOrdinalIsMutable();
          deviceOrdinal_.addInt(value);
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @param values The deviceOrdinal to add.
         * @return This builder for chaining.
         */
        public Builder addAllDeviceOrdinal(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureDeviceOrdinalIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, deviceOrdinal_);
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Virtual Device ordinal number determines the device ID of the device.
         * A Virtual device with a lower ordinal number always receives the a
         * smaller device id. The phyiscal device id and location in the
         * virtual device list is used to break ties.
         * </pre>
         *
         * <code>repeated int32 device_ordinal = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearDeviceOrdinal() {
          deviceOrdinal_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions.Experimental.VirtualDevices)
      }

      // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental.VirtualDevices)
      private static final org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices();
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<VirtualDevices>
          PARSER = new com.google.protobuf.AbstractParser<VirtualDevices>() {
        @java.lang.Override
        public VirtualDevices parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          Builder builder = newBuilder();
          try {
            builder.mergeFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(builder.buildPartial());
          } catch (com.google.protobuf.UninitializedMessageException e) {
            throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(e)
                .setUnfinishedMessage(builder.buildPartial());
          }
          return builder.buildPartial();
        }
      };

      public static com.google.protobuf.Parser<VirtualDevices> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<VirtualDevices> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface StreamMergeOptionsOrBuilder extends
        // @@protoc_insertion_point(interface_extends:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       * If true, the compute stream will be used for host_to_device copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream to finish. There is
       * also no need to wait for the copy to complete before executing the
       * callback function.
       * </pre>
       *
       * <code>bool merge_host_to_device_stream = 1;</code>
       * @return The mergeHostToDeviceStream.
       */
      boolean getMergeHostToDeviceStream();

      /**
       * <pre>
       * If true, the compute stream will be used for device_to_host copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream to finish.
       * </pre>
       *
       * <code>bool merge_device_to_host_stream = 2;</code>
       * @return The mergeDeviceToHostStream.
       */
      boolean getMergeDeviceToHostStream();

      /**
       * <pre>
       * If true, the compute stream will be used for device_to_device copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream of the sending device
       * to finish. There is also no need to wait for the compute stream of the
       * receiving device to finish if the copy is within the same device.
       * </pre>
       *
       * <code>bool merge_device_to_device_stream = 3;</code>
       * @return The mergeDeviceToDeviceStream.
       */
      boolean getMergeDeviceToDeviceStream();
    }
    /**
     * <pre>
     * Whether to merge data transfer streams into the compute stream in the
     * same stream group. Stream merging helps reduce the overhead caused by
     * stream synchronization, especially when data transfers are frequent. For
     * example, setting "merge_host_to_device_stream = true" will make the
     * compute stream responsible for both computation and host to device memory
     * copy.
     * </pre>
     *
     * Protobuf type {@code tensorflow.GPUOptions.Experimental.StreamMergeOptions}
     */
    public static final class StreamMergeOptions extends
        com.google.protobuf.GeneratedMessage implements
        // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
        StreamMergeOptionsOrBuilder {
    private static final long serialVersionUID = 0L;
      static {
        com.google.protobuf.RuntimeVersion.validateProtobufGencodeVersion(
          com.google.protobuf.RuntimeVersion.RuntimeDomain.PUBLIC,
          /* major= */ 4,
          /* minor= */ 28,
          /* patch= */ 3,
          /* suffix= */ "",
          StreamMergeOptions.class.getName());
      }
      // Use StreamMergeOptions.newBuilder() to construct.
      private StreamMergeOptions(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
        super(builder);
      }
      private StreamMergeOptions() {
      }

      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_StreamMergeOptions_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_StreamMergeOptions_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.class, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder.class);
      }

      public static final int MERGE_HOST_TO_DEVICE_STREAM_FIELD_NUMBER = 1;
      private boolean mergeHostToDeviceStream_ = false;
      /**
       * <pre>
       * If true, the compute stream will be used for host_to_device copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream to finish. There is
       * also no need to wait for the copy to complete before executing the
       * callback function.
       * </pre>
       *
       * <code>bool merge_host_to_device_stream = 1;</code>
       * @return The mergeHostToDeviceStream.
       */
      @java.lang.Override
      public boolean getMergeHostToDeviceStream() {
        return mergeHostToDeviceStream_;
      }

      public static final int MERGE_DEVICE_TO_HOST_STREAM_FIELD_NUMBER = 2;
      private boolean mergeDeviceToHostStream_ = false;
      /**
       * <pre>
       * If true, the compute stream will be used for device_to_host copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream to finish.
       * </pre>
       *
       * <code>bool merge_device_to_host_stream = 2;</code>
       * @return The mergeDeviceToHostStream.
       */
      @java.lang.Override
      public boolean getMergeDeviceToHostStream() {
        return mergeDeviceToHostStream_;
      }

      public static final int MERGE_DEVICE_TO_DEVICE_STREAM_FIELD_NUMBER = 3;
      private boolean mergeDeviceToDeviceStream_ = false;
      /**
       * <pre>
       * If true, the compute stream will be used for device_to_device copy as
       * well. It's no longer necessary to record an event before the copy to
       * let the copy stream wait for the compute stream of the sending device
       * to finish. There is also no need to wait for the compute stream of the
       * receiving device to finish if the copy is within the same device.
       * </pre>
       *
       * <code>bool merge_device_to_device_stream = 3;</code>
       * @return The mergeDeviceToDeviceStream.
       */
      @java.lang.Override
      public boolean getMergeDeviceToDeviceStream() {
        return mergeDeviceToDeviceStream_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (mergeHostToDeviceStream_ != false) {
          output.writeBool(1, mergeHostToDeviceStream_);
        }
        if (mergeDeviceToHostStream_ != false) {
          output.writeBool(2, mergeDeviceToHostStream_);
        }
        if (mergeDeviceToDeviceStream_ != false) {
          output.writeBool(3, mergeDeviceToDeviceStream_);
        }
        getUnknownFields().writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (mergeHostToDeviceStream_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(1, mergeHostToDeviceStream_);
        }
        if (mergeDeviceToHostStream_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(2, mergeDeviceToHostStream_);
        }
        if (mergeDeviceToDeviceStream_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(3, mergeDeviceToDeviceStream_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions)) {
          return super.equals(obj);
        }
        org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions other = (org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions) obj;

        if (getMergeHostToDeviceStream()
            != other.getMergeHostToDeviceStream()) return false;
        if (getMergeDeviceToHostStream()
            != other.getMergeDeviceToHostStream()) return false;
        if (getMergeDeviceToDeviceStream()
            != other.getMergeDeviceToDeviceStream()) return false;
        if (!getUnknownFields().equals(other.getUnknownFields())) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MERGE_HOST_TO_DEVICE_STREAM_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getMergeHostToDeviceStream());
        hash = (37 * hash) + MERGE_DEVICE_TO_HOST_STREAM_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getMergeDeviceToHostStream());
        hash = (37 * hash) + MERGE_DEVICE_TO_DEVICE_STREAM_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getMergeDeviceToDeviceStream());
        hash = (29 * hash) + getUnknownFields().hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseDelimitedWithIOException(PARSER, input);
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessage
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       * Whether to merge data transfer streams into the compute stream in the
       * same stream group. Stream merging helps reduce the overhead caused by
       * stream synchronization, especially when data transfers are frequent. For
       * example, setting "merge_host_to_device_stream = true" will make the
       * compute stream responsible for both computation and host to device memory
       * copy.
       * </pre>
       *
       * Protobuf type {@code tensorflow.GPUOptions.Experimental.StreamMergeOptions}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_StreamMergeOptions_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_StreamMergeOptions_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.class, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder.class);
        }

        // Construct using org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.newBuilder()
        private Builder() {

        }

        private Builder(
            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
          super(parent);

        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          bitField0_ = 0;
          mergeHostToDeviceStream_ = false;
          mergeDeviceToHostStream_ = false;
          mergeDeviceToDeviceStream_ = false;
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_StreamMergeOptions_descriptor;
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getDefaultInstanceForType() {
          return org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance();
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions build() {
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions buildPartial() {
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions result = new org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions(this);
          if (bitField0_ != 0) { buildPartial0(result); }
          onBuilt();
          return result;
        }

        private void buildPartial0(org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions result) {
          int from_bitField0_ = bitField0_;
          if (((from_bitField0_ & 0x00000001) != 0)) {
            result.mergeHostToDeviceStream_ = mergeHostToDeviceStream_;
          }
          if (((from_bitField0_ & 0x00000002) != 0)) {
            result.mergeDeviceToHostStream_ = mergeDeviceToHostStream_;
          }
          if (((from_bitField0_ & 0x00000004) != 0)) {
            result.mergeDeviceToDeviceStream_ = mergeDeviceToDeviceStream_;
          }
        }

        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions) {
            return mergeFrom((org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions other) {
          if (other == org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance()) return this;
          if (other.getMergeHostToDeviceStream() != false) {
            setMergeHostToDeviceStream(other.getMergeHostToDeviceStream());
          }
          if (other.getMergeDeviceToHostStream() != false) {
            setMergeDeviceToHostStream(other.getMergeDeviceToHostStream());
          }
          if (other.getMergeDeviceToDeviceStream() != false) {
            setMergeDeviceToDeviceStream(other.getMergeDeviceToDeviceStream());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 8: {
                  mergeHostToDeviceStream_ = input.readBool();
                  bitField0_ |= 0x00000001;
                  break;
                } // case 8
                case 16: {
                  mergeDeviceToHostStream_ = input.readBool();
                  bitField0_ |= 0x00000002;
                  break;
                } // case 16
                case 24: {
                  mergeDeviceToDeviceStream_ = input.readBool();
                  bitField0_ |= 0x00000004;
                  break;
                } // case 24
                default: {
                  if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                    done = true; // was an endgroup tag
                  }
                  break;
                } // default:
              } // switch (tag)
            } // while (!done)
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.unwrapIOException();
          } finally {
            onChanged();
          } // finally
          return this;
        }
        private int bitField0_;

        private boolean mergeHostToDeviceStream_ ;
        /**
         * <pre>
         * If true, the compute stream will be used for host_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish. There is
         * also no need to wait for the copy to complete before executing the
         * callback function.
         * </pre>
         *
         * <code>bool merge_host_to_device_stream = 1;</code>
         * @return The mergeHostToDeviceStream.
         */
        @java.lang.Override
        public boolean getMergeHostToDeviceStream() {
          return mergeHostToDeviceStream_;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for host_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish. There is
         * also no need to wait for the copy to complete before executing the
         * callback function.
         * </pre>
         *
         * <code>bool merge_host_to_device_stream = 1;</code>
         * @param value The mergeHostToDeviceStream to set.
         * @return This builder for chaining.
         */
        public Builder setMergeHostToDeviceStream(boolean value) {

          mergeHostToDeviceStream_ = value;
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for host_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish. There is
         * also no need to wait for the copy to complete before executing the
         * callback function.
         * </pre>
         *
         * <code>bool merge_host_to_device_stream = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMergeHostToDeviceStream() {
          bitField0_ = (bitField0_ & ~0x00000001);
          mergeHostToDeviceStream_ = false;
          onChanged();
          return this;
        }

        private boolean mergeDeviceToHostStream_ ;
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_host copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish.
         * </pre>
         *
         * <code>bool merge_device_to_host_stream = 2;</code>
         * @return The mergeDeviceToHostStream.
         */
        @java.lang.Override
        public boolean getMergeDeviceToHostStream() {
          return mergeDeviceToHostStream_;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_host copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish.
         * </pre>
         *
         * <code>bool merge_device_to_host_stream = 2;</code>
         * @param value The mergeDeviceToHostStream to set.
         * @return This builder for chaining.
         */
        public Builder setMergeDeviceToHostStream(boolean value) {

          mergeDeviceToHostStream_ = value;
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_host copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream to finish.
         * </pre>
         *
         * <code>bool merge_device_to_host_stream = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearMergeDeviceToHostStream() {
          bitField0_ = (bitField0_ & ~0x00000002);
          mergeDeviceToHostStream_ = false;
          onChanged();
          return this;
        }

        private boolean mergeDeviceToDeviceStream_ ;
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream of the sending device
         * to finish. There is also no need to wait for the compute stream of the
         * receiving device to finish if the copy is within the same device.
         * </pre>
         *
         * <code>bool merge_device_to_device_stream = 3;</code>
         * @return The mergeDeviceToDeviceStream.
         */
        @java.lang.Override
        public boolean getMergeDeviceToDeviceStream() {
          return mergeDeviceToDeviceStream_;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream of the sending device
         * to finish. There is also no need to wait for the compute stream of the
         * receiving device to finish if the copy is within the same device.
         * </pre>
         *
         * <code>bool merge_device_to_device_stream = 3;</code>
         * @param value The mergeDeviceToDeviceStream to set.
         * @return This builder for chaining.
         */
        public Builder setMergeDeviceToDeviceStream(boolean value) {

          mergeDeviceToDeviceStream_ = value;
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <pre>
         * If true, the compute stream will be used for device_to_device copy as
         * well. It's no longer necessary to record an event before the copy to
         * let the copy stream wait for the compute stream of the sending device
         * to finish. There is also no need to wait for the compute stream of the
         * receiving device to finish if the copy is within the same device.
         * </pre>
         *
         * <code>bool merge_device_to_device_stream = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearMergeDeviceToDeviceStream() {
          bitField0_ = (bitField0_ & ~0x00000004);
          mergeDeviceToDeviceStream_ = false;
          onChanged();
          return this;
        }

        // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
      }

      // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
      private static final org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions();
      }

      public static org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<StreamMergeOptions>
          PARSER = new com.google.protobuf.AbstractParser<StreamMergeOptions>() {
        @java.lang.Override
        public StreamMergeOptions parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          Builder builder = newBuilder();
          try {
            builder.mergeFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(builder.buildPartial());
          } catch (com.google.protobuf.UninitializedMessageException e) {
            throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(e)
                .setUnfinishedMessage(builder.buildPartial());
          }
          return builder.buildPartial();
        }
      };

      public static com.google.protobuf.Parser<StreamMergeOptions> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<StreamMergeOptions> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int bitField0_;
    public static final int VIRTUAL_DEVICES_FIELD_NUMBER = 1;
    @SuppressWarnings("serial")
    private java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> virtualDevices_;
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> getVirtualDevicesList() {
      return virtualDevices_;
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
        getVirtualDevicesOrBuilderList() {
      return virtualDevices_;
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    @java.lang.Override
    public int getVirtualDevicesCount() {
      return virtualDevices_.size();
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    @java.lang.Override
    public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index) {
      return virtualDevices_.get(index);
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order of the virtual devices determined by
     * device_ordinal and the location in the virtual device list.
     *
     * For example,
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
     *
     * but
     * visible_device_list = "1,0"
     * virtual_devices { memory_limit: 1GB memory_limit: 2GB
     * device_ordinal: 10 device_ordinal: 20}
     * virtual_devices { memory_limit: 3GB memory_limit: 4GB
     * device_ordinal: 10 device_ordinal: 20}
     * will create 4 virtual devices as:
     * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
     * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
     * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
     * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
     *
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     * at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     * different settings in different sessions within same process will
     * result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    @java.lang.Override
    public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
        int index) {
      return virtualDevices_.get(index);
    }

    public static final int NUM_VIRTUAL_DEVICES_PER_GPU_FIELD_NUMBER = 15;
    private int numVirtualDevicesPerGpu_ = 0;
    /**
     * <pre>
     * The number of virtual devices to create on each visible GPU. The
     * available memory will be split equally among all virtual devices. If the
     * field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
     * be ignored.
     * </pre>
     *
     * <code>int32 num_virtual_devices_per_gpu = 15;</code>
     * @return The numVirtualDevicesPerGpu.
     */
    @java.lang.Override
    public int getNumVirtualDevicesPerGpu() {
      return numVirtualDevicesPerGpu_;
    }

    public static final int USE_UNIFIED_MEMORY_FIELD_NUMBER = 2;
    private boolean useUnifiedMemory_ = false;
    /**
     * <pre>
     * If true, uses CUDA unified memory for memory allocations. If
     * per_process_gpu_memory_fraction option is greater than 1.0, then unified
     * memory is used regardless of the value for this field. See comments for
     * per_process_gpu_memory_fraction field for more details and requirements
     * of the unified memory. This option is useful to oversubscribe memory if
     * multiple processes are sharing a single GPU while individually using less
     * than 1.0 per process memory fraction.
     * </pre>
     *
     * <code>bool use_unified_memory = 2;</code>
     * @return The useUnifiedMemory.
     */
    @java.lang.Override
    public boolean getUseUnifiedMemory() {
      return useUnifiedMemory_;
    }

    public static final int NUM_DEV_TO_DEV_COPY_STREAMS_FIELD_NUMBER = 3;
    private int numDevToDevCopyStreams_ = 0;
    /**
     * <pre>
     * If &gt; 1, the number of device-to-device copy streams to create
     * for each GPUDevice.  Default value is 0, which is automatically
     * converted to 1.
     * </pre>
     *
     * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
     * @return The numDevToDevCopyStreams.
     */
    @java.lang.Override
    public int getNumDevToDevCopyStreams() {
      return numDevToDevCopyStreams_;
    }

    public static final int COLLECTIVE_RING_ORDER_FIELD_NUMBER = 4;
    @SuppressWarnings("serial")
    private volatile java.lang.Object collectiveRingOrder_ = "";
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     * @return The collectiveRingOrder.
     */
    @java.lang.Override
    public java.lang.String getCollectiveRingOrder() {
      java.lang.Object ref = collectiveRingOrder_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        collectiveRingOrder_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     * @return The bytes for collectiveRingOrder.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getCollectiveRingOrderBytes() {
      java.lang.Object ref = collectiveRingOrder_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        collectiveRingOrder_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TIMESTAMPED_ALLOCATOR_FIELD_NUMBER = 5;
    private boolean timestampedAllocator_ = false;
    /**
     * <pre>
     * If true then extra work is done by GPUDevice and GPUBFCAllocator to
     * keep track of when GPU memory is freed and when kernels actually
     * complete so that we can know when a nominally free memory chunk
     * is really not subject to pending use.
     * </pre>
     *
     * <code>bool timestamped_allocator = 5;</code>
     * @return The timestampedAllocator.
     */
    @java.lang.Override
    public boolean getTimestampedAllocator() {
      return timestampedAllocator_;
    }

    public static final int KERNEL_TRACKER_MAX_INTERVAL_FIELD_NUMBER = 7;
    private int kernelTrackerMaxInterval_ = 0;
    /**
     * <pre>
     * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
     * Note that timestamped_allocator is only effective if some tracking is
     * specified.
     *
     * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
     * is inserted after every n kernels without an event.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_interval = 7;</code>
     * @return The kernelTrackerMaxInterval.
     */
    @java.lang.Override
    public int getKernelTrackerMaxInterval() {
      return kernelTrackerMaxInterval_;
    }

    public static final int KERNEL_TRACKER_MAX_BYTES_FIELD_NUMBER = 8;
    private int kernelTrackerMaxBytes_ = 0;
    /**
     * <pre>
     * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
     * inserted after every series of kernels allocating a sum of
     * memory &gt;= n.  If one kernel allocates b * n bytes, then one
     * event will be inserted after it, but it will count as b against
     * the pending limit.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_bytes = 8;</code>
     * @return The kernelTrackerMaxBytes.
     */
    @java.lang.Override
    public int getKernelTrackerMaxBytes() {
      return kernelTrackerMaxBytes_;
    }

    public static final int KERNEL_TRACKER_MAX_PENDING_FIELD_NUMBER = 9;
    private int kernelTrackerMaxPending_ = 0;
    /**
     * <pre>
     * If kernel_tracker_max_pending &gt; 0 then no more than this many
     * tracking events can be outstanding at a time.  An attempt to
     * launch an additional kernel will stall until an event
     * completes.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_pending = 9;</code>
     * @return The kernelTrackerMaxPending.
     */
    @java.lang.Override
    public int getKernelTrackerMaxPending() {
      return kernelTrackerMaxPending_;
    }

    public static final int INTERNAL_FRAGMENTATION_FRACTION_FIELD_NUMBER = 10;
    private double internalFragmentationFraction_ = 0D;
    /**
     * <pre>
     * BFC Allocator can return an allocated chunk of memory upto 2x the
     * requested size. For virtual devices with tight memory constraints, and
     * proportionately large allocation requests, this can lead to a significant
     * reduction in available memory. The threshold below controls when a chunk
     * should be split if the chunk size exceeds requested memory size. It is
     * expressed as a fraction of total available memory for the tf device. For
     * example setting it to 0.05 would imply a chunk needs to be split if its
     * size exceeds the requested memory by 5% of the total virtual device/gpu
     * memory size.
     * </pre>
     *
     * <code>double internal_fragmentation_fraction = 10;</code>
     * @return The internalFragmentationFraction.
     */
    @java.lang.Override
    public double getInternalFragmentationFraction() {
      return internalFragmentationFraction_;
    }

    public static final int USE_CUDA_MALLOC_ASYNC_FIELD_NUMBER = 11;
    private boolean useCudaMallocAsync_ = false;
    /**
     * <pre>
     * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
     * </pre>
     *
     * <code>bool use_cuda_malloc_async = 11;</code>
     * @return The useCudaMallocAsync.
     */
    @java.lang.Override
    public boolean getUseCudaMallocAsync() {
      return useCudaMallocAsync_;
    }

    public static final int DISALLOW_RETRY_ON_ALLOCATION_FAILURE_FIELD_NUMBER = 12;
    private boolean disallowRetryOnAllocationFailure_ = false;
    /**
     * <pre>
     * By default, BFCAllocator may sleep when it runs out of memory, in the
     * hopes that another thread will free up memory in the meantime.  Setting
     * this to true disables the sleep; instead we'll OOM immediately.
     * </pre>
     *
     * <code>bool disallow_retry_on_allocation_failure = 12;</code>
     * @return The disallowRetryOnAllocationFailure.
     */
    @java.lang.Override
    public boolean getDisallowRetryOnAllocationFailure() {
      return disallowRetryOnAllocationFailure_;
    }

    public static final int GPU_HOST_MEM_LIMIT_IN_MB_FIELD_NUMBER = 13;
    private float gpuHostMemLimitInMb_ = 0F;
    /**
     * <pre>
     * Memory limit for "GPU host allocator", aka pinned memory allocator.  This
     * can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
     * </pre>
     *
     * <code>float gpu_host_mem_limit_in_mb = 13;</code>
     * @return The gpuHostMemLimitInMb.
     */
    @java.lang.Override
    public float getGpuHostMemLimitInMb() {
      return gpuHostMemLimitInMb_;
    }

    public static final int GPU_HOST_MEM_DISALLOW_GROWTH_FIELD_NUMBER = 14;
    private boolean gpuHostMemDisallowGrowth_ = false;
    /**
     * <pre>
     * If true, then the host allocator allocates its max memory all upfront and
     * never grows.  This can be useful for latency-sensitive systems, because
     * growing the GPU host memory pool can be expensive.
     *
     * You probably only want to use this in combination with
     * gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
     * quite high.
     * </pre>
     *
     * <code>bool gpu_host_mem_disallow_growth = 14;</code>
     * @return The gpuHostMemDisallowGrowth.
     */
    @java.lang.Override
    public boolean getGpuHostMemDisallowGrowth() {
      return gpuHostMemDisallowGrowth_;
    }

    public static final int GPU_SYSTEM_MEMORY_SIZE_IN_MB_FIELD_NUMBER = 16;
    private int gpuSystemMemorySizeInMb_ = 0;
    /**
     * <pre>
     * Memory limit for gpu system. This can also be set by
     * TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
     * gpu_system_memory_size_in_mb. With this, user can configure the gpu
     * system memory size for better resource estimation of multi-tenancy(one
     * gpu with multiple model) use case.
     * </pre>
     *
     * <code>int32 gpu_system_memory_size_in_mb = 16;</code>
     * @return The gpuSystemMemorySizeInMb.
     */
    @java.lang.Override
    public int getGpuSystemMemorySizeInMb() {
      return gpuSystemMemorySizeInMb_;
    }

    public static final int POPULATE_PJRT_GPU_CLIENT_CREATION_INFO_FIELD_NUMBER = 17;
    private boolean populatePjrtGpuClientCreationInfo_ = false;
    /**
     * <pre>
     * If true, save information needed for created a PjRt GPU client for
     * creating a client with remote devices.
     * </pre>
     *
     * <code>bool populate_pjrt_gpu_client_creation_info = 17;</code>
     * @return The populatePjrtGpuClientCreationInfo.
     */
    @java.lang.Override
    public boolean getPopulatePjrtGpuClientCreationInfo() {
      return populatePjrtGpuClientCreationInfo_;
    }

    public static final int NODE_ID_FIELD_NUMBER = 18;
    private int nodeId_ = 0;
    /**
     * <pre>
     * node_id for use when creating a PjRt GPU client with remote devices,
     * which enumerates jobs*tasks from a ServerDef.
     * </pre>
     *
     * <code>int32 node_id = 18;</code>
     * @return The nodeId.
     */
    @java.lang.Override
    public int getNodeId() {
      return nodeId_;
    }

    public static final int STREAM_MERGE_OPTIONS_FIELD_NUMBER = 19;
    private org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions streamMergeOptions_;
    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     * @return Whether the streamMergeOptions field is set.
     */
    @java.lang.Override
    public boolean hasStreamMergeOptions() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     * @return The streamMergeOptions.
     */
    @java.lang.Override
    public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getStreamMergeOptions() {
      return streamMergeOptions_ == null ? org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance() : streamMergeOptions_;
    }
    /**
     * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
     */
    @java.lang.Override
    public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder getStreamMergeOptionsOrBuilder() {
      return streamMergeOptions_ == null ? org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance() : streamMergeOptions_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < virtualDevices_.size(); i++) {
        output.writeMessage(1, virtualDevices_.get(i));
      }
      if (useUnifiedMemory_ != false) {
        output.writeBool(2, useUnifiedMemory_);
      }
      if (numDevToDevCopyStreams_ != 0) {
        output.writeInt32(3, numDevToDevCopyStreams_);
      }
      if (!com.google.protobuf.GeneratedMessage.isStringEmpty(collectiveRingOrder_)) {
        com.google.protobuf.GeneratedMessage.writeString(output, 4, collectiveRingOrder_);
      }
      if (timestampedAllocator_ != false) {
        output.writeBool(5, timestampedAllocator_);
      }
      if (kernelTrackerMaxInterval_ != 0) {
        output.writeInt32(7, kernelTrackerMaxInterval_);
      }
      if (kernelTrackerMaxBytes_ != 0) {
        output.writeInt32(8, kernelTrackerMaxBytes_);
      }
      if (kernelTrackerMaxPending_ != 0) {
        output.writeInt32(9, kernelTrackerMaxPending_);
      }
      if (java.lang.Double.doubleToRawLongBits(internalFragmentationFraction_) != 0) {
        output.writeDouble(10, internalFragmentationFraction_);
      }
      if (useCudaMallocAsync_ != false) {
        output.writeBool(11, useCudaMallocAsync_);
      }
      if (disallowRetryOnAllocationFailure_ != false) {
        output.writeBool(12, disallowRetryOnAllocationFailure_);
      }
      if (java.lang.Float.floatToRawIntBits(gpuHostMemLimitInMb_) != 0) {
        output.writeFloat(13, gpuHostMemLimitInMb_);
      }
      if (gpuHostMemDisallowGrowth_ != false) {
        output.writeBool(14, gpuHostMemDisallowGrowth_);
      }
      if (numVirtualDevicesPerGpu_ != 0) {
        output.writeInt32(15, numVirtualDevicesPerGpu_);
      }
      if (gpuSystemMemorySizeInMb_ != 0) {
        output.writeInt32(16, gpuSystemMemorySizeInMb_);
      }
      if (populatePjrtGpuClientCreationInfo_ != false) {
        output.writeBool(17, populatePjrtGpuClientCreationInfo_);
      }
      if (nodeId_ != 0) {
        output.writeInt32(18, nodeId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(19, getStreamMergeOptions());
      }
      getUnknownFields().writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < virtualDevices_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, virtualDevices_.get(i));
      }
      if (useUnifiedMemory_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, useUnifiedMemory_);
      }
      if (numDevToDevCopyStreams_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, numDevToDevCopyStreams_);
      }
      if (!com.google.protobuf.GeneratedMessage.isStringEmpty(collectiveRingOrder_)) {
        size += com.google.protobuf.GeneratedMessage.computeStringSize(4, collectiveRingOrder_);
      }
      if (timestampedAllocator_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, timestampedAllocator_);
      }
      if (kernelTrackerMaxInterval_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, kernelTrackerMaxInterval_);
      }
      if (kernelTrackerMaxBytes_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(8, kernelTrackerMaxBytes_);
      }
      if (kernelTrackerMaxPending_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, kernelTrackerMaxPending_);
      }
      if (java.lang.Double.doubleToRawLongBits(internalFragmentationFraction_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(10, internalFragmentationFraction_);
      }
      if (useCudaMallocAsync_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(11, useCudaMallocAsync_);
      }
      if (disallowRetryOnAllocationFailure_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(12, disallowRetryOnAllocationFailure_);
      }
      if (java.lang.Float.floatToRawIntBits(gpuHostMemLimitInMb_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(13, gpuHostMemLimitInMb_);
      }
      if (gpuHostMemDisallowGrowth_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(14, gpuHostMemDisallowGrowth_);
      }
      if (numVirtualDevicesPerGpu_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(15, numVirtualDevicesPerGpu_);
      }
      if (gpuSystemMemorySizeInMb_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(16, gpuSystemMemorySizeInMb_);
      }
      if (populatePjrtGpuClientCreationInfo_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(17, populatePjrtGpuClientCreationInfo_);
      }
      if (nodeId_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(18, nodeId_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(19, getStreamMergeOptions());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.tensorflow.proto.GPUOptions.Experimental)) {
        return super.equals(obj);
      }
      org.tensorflow.proto.GPUOptions.Experimental other = (org.tensorflow.proto.GPUOptions.Experimental) obj;

      if (!getVirtualDevicesList()
          .equals(other.getVirtualDevicesList())) return false;
      if (getNumVirtualDevicesPerGpu()
          != other.getNumVirtualDevicesPerGpu()) return false;
      if (getUseUnifiedMemory()
          != other.getUseUnifiedMemory()) return false;
      if (getNumDevToDevCopyStreams()
          != other.getNumDevToDevCopyStreams()) return false;
      if (!getCollectiveRingOrder()
          .equals(other.getCollectiveRingOrder())) return false;
      if (getTimestampedAllocator()
          != other.getTimestampedAllocator()) return false;
      if (getKernelTrackerMaxInterval()
          != other.getKernelTrackerMaxInterval()) return false;
      if (getKernelTrackerMaxBytes()
          != other.getKernelTrackerMaxBytes()) return false;
      if (getKernelTrackerMaxPending()
          != other.getKernelTrackerMaxPending()) return false;
      if (java.lang.Double.doubleToLongBits(getInternalFragmentationFraction())
          != java.lang.Double.doubleToLongBits(
              other.getInternalFragmentationFraction())) return false;
      if (getUseCudaMallocAsync()
          != other.getUseCudaMallocAsync()) return false;
      if (getDisallowRetryOnAllocationFailure()
          != other.getDisallowRetryOnAllocationFailure()) return false;
      if (java.lang.Float.floatToIntBits(getGpuHostMemLimitInMb())
          != java.lang.Float.floatToIntBits(
              other.getGpuHostMemLimitInMb())) return false;
      if (getGpuHostMemDisallowGrowth()
          != other.getGpuHostMemDisallowGrowth()) return false;
      if (getGpuSystemMemorySizeInMb()
          != other.getGpuSystemMemorySizeInMb()) return false;
      if (getPopulatePjrtGpuClientCreationInfo()
          != other.getPopulatePjrtGpuClientCreationInfo()) return false;
      if (getNodeId()
          != other.getNodeId()) return false;
      if (hasStreamMergeOptions() != other.hasStreamMergeOptions()) return false;
      if (hasStreamMergeOptions()) {
        if (!getStreamMergeOptions()
            .equals(other.getStreamMergeOptions())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getVirtualDevicesCount() > 0) {
        hash = (37 * hash) + VIRTUAL_DEVICES_FIELD_NUMBER;
        hash = (53 * hash) + getVirtualDevicesList().hashCode();
      }
      hash = (37 * hash) + NUM_VIRTUAL_DEVICES_PER_GPU_FIELD_NUMBER;
      hash = (53 * hash) + getNumVirtualDevicesPerGpu();
      hash = (37 * hash) + USE_UNIFIED_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseUnifiedMemory());
      hash = (37 * hash) + NUM_DEV_TO_DEV_COPY_STREAMS_FIELD_NUMBER;
      hash = (53 * hash) + getNumDevToDevCopyStreams();
      hash = (37 * hash) + COLLECTIVE_RING_ORDER_FIELD_NUMBER;
      hash = (53 * hash) + getCollectiveRingOrder().hashCode();
      hash = (37 * hash) + TIMESTAMPED_ALLOCATOR_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getTimestampedAllocator());
      hash = (37 * hash) + KERNEL_TRACKER_MAX_INTERVAL_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxInterval();
      hash = (37 * hash) + KERNEL_TRACKER_MAX_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxBytes();
      hash = (37 * hash) + KERNEL_TRACKER_MAX_PENDING_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxPending();
      hash = (37 * hash) + INTERNAL_FRAGMENTATION_FRACTION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getInternalFragmentationFraction()));
      hash = (37 * hash) + USE_CUDA_MALLOC_ASYNC_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseCudaMallocAsync());
      hash = (37 * hash) + DISALLOW_RETRY_ON_ALLOCATION_FAILURE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDisallowRetryOnAllocationFailure());
      hash = (37 * hash) + GPU_HOST_MEM_LIMIT_IN_MB_FIELD_NUMBER;
      hash = (53 * hash) + java.lang.Float.floatToIntBits(
          getGpuHostMemLimitInMb());
      hash = (37 * hash) + GPU_HOST_MEM_DISALLOW_GROWTH_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getGpuHostMemDisallowGrowth());
      hash = (37 * hash) + GPU_SYSTEM_MEMORY_SIZE_IN_MB_FIELD_NUMBER;
      hash = (53 * hash) + getGpuSystemMemorySizeInMb();
      hash = (37 * hash) + POPULATE_PJRT_GPU_CLIENT_CREATION_INFO_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getPopulatePjrtGpuClientCreationInfo());
      hash = (37 * hash) + NODE_ID_FIELD_NUMBER;
      hash = (53 * hash) + getNodeId();
      if (hasStreamMergeOptions()) {
        hash = (37 * hash) + STREAM_MERGE_OPTIONS_FIELD_NUMBER;
        hash = (53 * hash) + getStreamMergeOptions().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.tensorflow.proto.GPUOptions.Experimental parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.tensorflow.proto.GPUOptions.Experimental parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.GPUOptions.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessage
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.tensorflow.proto.GPUOptions.Experimental prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code tensorflow.GPUOptions.Experimental}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions.Experimental)
        org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.GPUOptions.Experimental.class, org.tensorflow.proto.GPUOptions.Experimental.Builder.class);
      }

      // Construct using org.tensorflow.proto.GPUOptions.Experimental.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage
                .alwaysUseFieldBuilders) {
          getVirtualDevicesFieldBuilder();
          getStreamMergeOptionsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        if (virtualDevicesBuilder_ == null) {
          virtualDevices_ = java.util.Collections.emptyList();
        } else {
          virtualDevices_ = null;
          virtualDevicesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        numVirtualDevicesPerGpu_ = 0;
        useUnifiedMemory_ = false;
        numDevToDevCopyStreams_ = 0;
        collectiveRingOrder_ = "";
        timestampedAllocator_ = false;
        kernelTrackerMaxInterval_ = 0;
        kernelTrackerMaxBytes_ = 0;
        kernelTrackerMaxPending_ = 0;
        internalFragmentationFraction_ = 0D;
        useCudaMallocAsync_ = false;
        disallowRetryOnAllocationFailure_ = false;
        gpuHostMemLimitInMb_ = 0F;
        gpuHostMemDisallowGrowth_ = false;
        gpuSystemMemorySizeInMb_ = 0;
        populatePjrtGpuClientCreationInfo_ = false;
        nodeId_ = 0;
        streamMergeOptions_ = null;
        if (streamMergeOptionsBuilder_ != null) {
          streamMergeOptionsBuilder_.dispose();
          streamMergeOptionsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
      }

      @java.lang.Override
      public org.tensorflow.proto.GPUOptions.Experimental getDefaultInstanceForType() {
        return org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance();
      }

      @java.lang.Override
      public org.tensorflow.proto.GPUOptions.Experimental build() {
        org.tensorflow.proto.GPUOptions.Experimental result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.tensorflow.proto.GPUOptions.Experimental buildPartial() {
        org.tensorflow.proto.GPUOptions.Experimental result = new org.tensorflow.proto.GPUOptions.Experimental(this);
        buildPartialRepeatedFields(result);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartialRepeatedFields(org.tensorflow.proto.GPUOptions.Experimental result) {
        if (virtualDevicesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            virtualDevices_ = java.util.Collections.unmodifiableList(virtualDevices_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.virtualDevices_ = virtualDevices_;
        } else {
          result.virtualDevices_ = virtualDevicesBuilder_.build();
        }
      }

      private void buildPartial0(org.tensorflow.proto.GPUOptions.Experimental result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.numVirtualDevicesPerGpu_ = numVirtualDevicesPerGpu_;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.useUnifiedMemory_ = useUnifiedMemory_;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.numDevToDevCopyStreams_ = numDevToDevCopyStreams_;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.collectiveRingOrder_ = collectiveRingOrder_;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.timestampedAllocator_ = timestampedAllocator_;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.kernelTrackerMaxInterval_ = kernelTrackerMaxInterval_;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          result.kernelTrackerMaxBytes_ = kernelTrackerMaxBytes_;
        }
        if (((from_bitField0_ & 0x00000100) != 0)) {
          result.kernelTrackerMaxPending_ = kernelTrackerMaxPending_;
        }
        if (((from_bitField0_ & 0x00000200) != 0)) {
          result.internalFragmentationFraction_ = internalFragmentationFraction_;
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          result.useCudaMallocAsync_ = useCudaMallocAsync_;
        }
        if (((from_bitField0_ & 0x00000800) != 0)) {
          result.disallowRetryOnAllocationFailure_ = disallowRetryOnAllocationFailure_;
        }
        if (((from_bitField0_ & 0x00001000) != 0)) {
          result.gpuHostMemLimitInMb_ = gpuHostMemLimitInMb_;
        }
        if (((from_bitField0_ & 0x00002000) != 0)) {
          result.gpuHostMemDisallowGrowth_ = gpuHostMemDisallowGrowth_;
        }
        if (((from_bitField0_ & 0x00004000) != 0)) {
          result.gpuSystemMemorySizeInMb_ = gpuSystemMemorySizeInMb_;
        }
        if (((from_bitField0_ & 0x00008000) != 0)) {
          result.populatePjrtGpuClientCreationInfo_ = populatePjrtGpuClientCreationInfo_;
        }
        if (((from_bitField0_ & 0x00010000) != 0)) {
          result.nodeId_ = nodeId_;
        }
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00020000) != 0)) {
          result.streamMergeOptions_ = streamMergeOptionsBuilder_ == null
              ? streamMergeOptions_
              : streamMergeOptionsBuilder_.build();
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ |= to_bitField0_;
      }

      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.tensorflow.proto.GPUOptions.Experimental) {
          return mergeFrom((org.tensorflow.proto.GPUOptions.Experimental)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.tensorflow.proto.GPUOptions.Experimental other) {
        if (other == org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance()) return this;
        if (virtualDevicesBuilder_ == null) {
          if (!other.virtualDevices_.isEmpty()) {
            if (virtualDevices_.isEmpty()) {
              virtualDevices_ = other.virtualDevices_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureVirtualDevicesIsMutable();
              virtualDevices_.addAll(other.virtualDevices_);
            }
            onChanged();
          }
        } else {
          if (!other.virtualDevices_.isEmpty()) {
            if (virtualDevicesBuilder_.isEmpty()) {
              virtualDevicesBuilder_.dispose();
              virtualDevicesBuilder_ = null;
              virtualDevices_ = other.virtualDevices_;
              bitField0_ = (bitField0_ & ~0x00000001);
              virtualDevicesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getVirtualDevicesFieldBuilder() : null;
            } else {
              virtualDevicesBuilder_.addAllMessages(other.virtualDevices_);
            }
          }
        }
        if (other.getNumVirtualDevicesPerGpu() != 0) {
          setNumVirtualDevicesPerGpu(other.getNumVirtualDevicesPerGpu());
        }
        if (other.getUseUnifiedMemory() != false) {
          setUseUnifiedMemory(other.getUseUnifiedMemory());
        }
        if (other.getNumDevToDevCopyStreams() != 0) {
          setNumDevToDevCopyStreams(other.getNumDevToDevCopyStreams());
        }
        if (!other.getCollectiveRingOrder().isEmpty()) {
          collectiveRingOrder_ = other.collectiveRingOrder_;
          bitField0_ |= 0x00000010;
          onChanged();
        }
        if (other.getTimestampedAllocator() != false) {
          setTimestampedAllocator(other.getTimestampedAllocator());
        }
        if (other.getKernelTrackerMaxInterval() != 0) {
          setKernelTrackerMaxInterval(other.getKernelTrackerMaxInterval());
        }
        if (other.getKernelTrackerMaxBytes() != 0) {
          setKernelTrackerMaxBytes(other.getKernelTrackerMaxBytes());
        }
        if (other.getKernelTrackerMaxPending() != 0) {
          setKernelTrackerMaxPending(other.getKernelTrackerMaxPending());
        }
        if (other.getInternalFragmentationFraction() != 0D) {
          setInternalFragmentationFraction(other.getInternalFragmentationFraction());
        }
        if (other.getUseCudaMallocAsync() != false) {
          setUseCudaMallocAsync(other.getUseCudaMallocAsync());
        }
        if (other.getDisallowRetryOnAllocationFailure() != false) {
          setDisallowRetryOnAllocationFailure(other.getDisallowRetryOnAllocationFailure());
        }
        if (other.getGpuHostMemLimitInMb() != 0F) {
          setGpuHostMemLimitInMb(other.getGpuHostMemLimitInMb());
        }
        if (other.getGpuHostMemDisallowGrowth() != false) {
          setGpuHostMemDisallowGrowth(other.getGpuHostMemDisallowGrowth());
        }
        if (other.getGpuSystemMemorySizeInMb() != 0) {
          setGpuSystemMemorySizeInMb(other.getGpuSystemMemorySizeInMb());
        }
        if (other.getPopulatePjrtGpuClientCreationInfo() != false) {
          setPopulatePjrtGpuClientCreationInfo(other.getPopulatePjrtGpuClientCreationInfo());
        }
        if (other.getNodeId() != 0) {
          setNodeId(other.getNodeId());
        }
        if (other.hasStreamMergeOptions()) {
          mergeStreamMergeOptions(other.getStreamMergeOptions());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices m =
                    input.readMessage(
                        org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.parser(),
                        extensionRegistry);
                if (virtualDevicesBuilder_ == null) {
                  ensureVirtualDevicesIsMutable();
                  virtualDevices_.add(m);
                } else {
                  virtualDevicesBuilder_.addMessage(m);
                }
                break;
              } // case 10
              case 16: {
                useUnifiedMemory_ = input.readBool();
                bitField0_ |= 0x00000004;
                break;
              } // case 16
              case 24: {
                numDevToDevCopyStreams_ = input.readInt32();
                bitField0_ |= 0x00000008;
                break;
              } // case 24
              case 34: {
                collectiveRingOrder_ = input.readStringRequireUtf8();
                bitField0_ |= 0x00000010;
                break;
              } // case 34
              case 40: {
                timestampedAllocator_ = input.readBool();
                bitField0_ |= 0x00000020;
                break;
              } // case 40
              case 56: {
                kernelTrackerMaxInterval_ = input.readInt32();
                bitField0_ |= 0x00000040;
                break;
              } // case 56
              case 64: {
                kernelTrackerMaxBytes_ = input.readInt32();
                bitField0_ |= 0x00000080;
                break;
              } // case 64
              case 72: {
                kernelTrackerMaxPending_ = input.readInt32();
                bitField0_ |= 0x00000100;
                break;
              } // case 72
              case 81: {
                internalFragmentationFraction_ = input.readDouble();
                bitField0_ |= 0x00000200;
                break;
              } // case 81
              case 88: {
                useCudaMallocAsync_ = input.readBool();
                bitField0_ |= 0x00000400;
                break;
              } // case 88
              case 96: {
                disallowRetryOnAllocationFailure_ = input.readBool();
                bitField0_ |= 0x00000800;
                break;
              } // case 96
              case 109: {
                gpuHostMemLimitInMb_ = input.readFloat();
                bitField0_ |= 0x00001000;
                break;
              } // case 109
              case 112: {
                gpuHostMemDisallowGrowth_ = input.readBool();
                bitField0_ |= 0x00002000;
                break;
              } // case 112
              case 120: {
                numVirtualDevicesPerGpu_ = input.readInt32();
                bitField0_ |= 0x00000002;
                break;
              } // case 120
              case 128: {
                gpuSystemMemorySizeInMb_ = input.readInt32();
                bitField0_ |= 0x00004000;
                break;
              } // case 128
              case 136: {
                populatePjrtGpuClientCreationInfo_ = input.readBool();
                bitField0_ |= 0x00008000;
                break;
              } // case 136
              case 144: {
                nodeId_ = input.readInt32();
                bitField0_ |= 0x00010000;
                break;
              } // case 144
              case 154: {
                input.readMessage(
                    getStreamMergeOptionsFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00020000;
                break;
              } // case 154
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> virtualDevices_ =
        java.util.Collections.emptyList();
      private void ensureVirtualDevicesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          virtualDevices_ = new java.util.ArrayList<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices>(virtualDevices_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder> virtualDevicesBuilder_;

      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> getVirtualDevicesList() {
        if (virtualDevicesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(virtualDevices_);
        } else {
          return virtualDevicesBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public int getVirtualDevicesCount() {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.size();
        } else {
          return virtualDevicesBuilder_.getCount();
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index) {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.get(index);
        } else {
          return virtualDevicesBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder setVirtualDevices(
          int index, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.set(index, value);
          onChanged();
        } else {
          virtualDevicesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder setVirtualDevices(
          int index, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.set(index, builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(value);
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          int index, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(index, value);
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          int index, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(index, builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addAllVirtualDevices(
          java.lang.Iterable<? extends org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices> values) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, virtualDevices_);
          onChanged();
        } else {
          virtualDevicesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder clearVirtualDevices() {
        if (virtualDevicesBuilder_ == null) {
          virtualDevices_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          virtualDevicesBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder removeVirtualDevices(int index) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.remove(index);
          onChanged();
        } else {
          virtualDevicesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder getVirtualDevicesBuilder(
          int index) {
        return getVirtualDevicesFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
          int index) {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.get(index);  } else {
          return virtualDevicesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<? extends org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
           getVirtualDevicesOrBuilderList() {
        if (virtualDevicesBuilder_ != null) {
          return virtualDevicesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(virtualDevices_);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder addVirtualDevicesBuilder() {
        return getVirtualDevicesFieldBuilder().addBuilder(
            org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.getDefaultInstance());
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder addVirtualDevicesBuilder(
          int index) {
        return getVirtualDevicesFieldBuilder().addBuilder(
            index, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.getDefaultInstance());
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order of the virtual devices determined by
       * device_ordinal and the location in the virtual device list.
       *
       * For example,
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB }
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       * /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       * /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
       *
       * but
       * visible_device_list = "1,0"
       * virtual_devices { memory_limit: 1GB memory_limit: 2GB
       * device_ordinal: 10 device_ordinal: 20}
       * virtual_devices { memory_limit: 3GB memory_limit: 4GB
       * device_ordinal: 10 device_ordinal: 20}
       * will create 4 virtual devices as:
       * /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
       * /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
       * /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
       * /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
       *
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       * at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       * different settings in different sessions within same process will
       * result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder> 
           getVirtualDevicesBuilderList() {
        return getVirtualDevicesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
          getVirtualDevicesFieldBuilder() {
        if (virtualDevicesBuilder_ == null) {
          virtualDevicesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.GPUOptions.Experimental.VirtualDevicesOrBuilder>(
                  virtualDevices_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          virtualDevices_ = null;
        }
        return virtualDevicesBuilder_;
      }

      private int numVirtualDevicesPerGpu_ ;
      /**
       * <pre>
       * The number of virtual devices to create on each visible GPU. The
       * available memory will be split equally among all virtual devices. If the
       * field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
       * be ignored.
       * </pre>
       *
       * <code>int32 num_virtual_devices_per_gpu = 15;</code>
       * @return The numVirtualDevicesPerGpu.
       */
      @java.lang.Override
      public int getNumVirtualDevicesPerGpu() {
        return numVirtualDevicesPerGpu_;
      }
      /**
       * <pre>
       * The number of virtual devices to create on each visible GPU. The
       * available memory will be split equally among all virtual devices. If the
       * field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
       * be ignored.
       * </pre>
       *
       * <code>int32 num_virtual_devices_per_gpu = 15;</code>
       * @param value The numVirtualDevicesPerGpu to set.
       * @return This builder for chaining.
       */
      public Builder setNumVirtualDevicesPerGpu(int value) {

        numVirtualDevicesPerGpu_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The number of virtual devices to create on each visible GPU. The
       * available memory will be split equally among all virtual devices. If the
       * field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
       * be ignored.
       * </pre>
       *
       * <code>int32 num_virtual_devices_per_gpu = 15;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumVirtualDevicesPerGpu() {
        bitField0_ = (bitField0_ & ~0x00000002);
        numVirtualDevicesPerGpu_ = 0;
        onChanged();
        return this;
      }

      private boolean useUnifiedMemory_ ;
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       * @return The useUnifiedMemory.
       */
      @java.lang.Override
      public boolean getUseUnifiedMemory() {
        return useUnifiedMemory_;
      }
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       * @param value The useUnifiedMemory to set.
       * @return This builder for chaining.
       */
      public Builder setUseUnifiedMemory(boolean value) {

        useUnifiedMemory_ = value;
        bitField0_ |= 0x00000004;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearUseUnifiedMemory() {
        bitField0_ = (bitField0_ & ~0x00000004);
        useUnifiedMemory_ = false;
        onChanged();
        return this;
      }

      private int numDevToDevCopyStreams_ ;
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       * @return The numDevToDevCopyStreams.
       */
      @java.lang.Override
      public int getNumDevToDevCopyStreams() {
        return numDevToDevCopyStreams_;
      }
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       * @param value The numDevToDevCopyStreams to set.
       * @return This builder for chaining.
       */
      public Builder setNumDevToDevCopyStreams(int value) {

        numDevToDevCopyStreams_ = value;
        bitField0_ |= 0x00000008;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumDevToDevCopyStreams() {
        bitField0_ = (bitField0_ & ~0x00000008);
        numDevToDevCopyStreams_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object collectiveRingOrder_ = "";
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       * @return The collectiveRingOrder.
       */
      public java.lang.String getCollectiveRingOrder() {
        java.lang.Object ref = collectiveRingOrder_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          collectiveRingOrder_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       * @return The bytes for collectiveRingOrder.
       */
      public com.google.protobuf.ByteString
          getCollectiveRingOrderBytes() {
        java.lang.Object ref = collectiveRingOrder_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          collectiveRingOrder_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       * @param value The collectiveRingOrder to set.
       * @return This builder for chaining.
       */
      public Builder setCollectiveRingOrder(
          java.lang.String value) {
        if (value == null) { throw new NullPointerException(); }
        collectiveRingOrder_ = value;
        bitField0_ |= 0x00000010;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearCollectiveRingOrder() {
        collectiveRingOrder_ = getDefaultInstance().getCollectiveRingOrder();
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       * @param value The bytes for collectiveRingOrder to set.
       * @return This builder for chaining.
       */
      public Builder setCollectiveRingOrderBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        checkByteStringIsUtf8(value);
        collectiveRingOrder_ = value;
        bitField0_ |= 0x00000010;
        onChanged();
        return this;
      }

      private boolean timestampedAllocator_ ;
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       * @return The timestampedAllocator.
       */
      @java.lang.Override
      public boolean getTimestampedAllocator() {
        return timestampedAllocator_;
      }
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       * @param value The timestampedAllocator to set.
       * @return This builder for chaining.
       */
      public Builder setTimestampedAllocator(boolean value) {

        timestampedAllocator_ = value;
        bitField0_ |= 0x00000020;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearTimestampedAllocator() {
        bitField0_ = (bitField0_ & ~0x00000020);
        timestampedAllocator_ = false;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxInterval_ ;
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       *
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       * @return The kernelTrackerMaxInterval.
       */
      @java.lang.Override
      public int getKernelTrackerMaxInterval() {
        return kernelTrackerMaxInterval_;
      }
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       *
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       * @param value The kernelTrackerMaxInterval to set.
       * @return This builder for chaining.
       */
      public Builder setKernelTrackerMaxInterval(int value) {

        kernelTrackerMaxInterval_ = value;
        bitField0_ |= 0x00000040;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       *
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearKernelTrackerMaxInterval() {
        bitField0_ = (bitField0_ & ~0x00000040);
        kernelTrackerMaxInterval_ = 0;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxBytes_ ;
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       * @return The kernelTrackerMaxBytes.
       */
      @java.lang.Override
      public int getKernelTrackerMaxBytes() {
        return kernelTrackerMaxBytes_;
      }
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       * @param value The kernelTrackerMaxBytes to set.
       * @return This builder for chaining.
       */
      public Builder setKernelTrackerMaxBytes(int value) {

        kernelTrackerMaxBytes_ = value;
        bitField0_ |= 0x00000080;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearKernelTrackerMaxBytes() {
        bitField0_ = (bitField0_ & ~0x00000080);
        kernelTrackerMaxBytes_ = 0;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxPending_ ;
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       * @return The kernelTrackerMaxPending.
       */
      @java.lang.Override
      public int getKernelTrackerMaxPending() {
        return kernelTrackerMaxPending_;
      }
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       * @param value The kernelTrackerMaxPending to set.
       * @return This builder for chaining.
       */
      public Builder setKernelTrackerMaxPending(int value) {

        kernelTrackerMaxPending_ = value;
        bitField0_ |= 0x00000100;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       * @return This builder for chaining.
       */
      public Builder clearKernelTrackerMaxPending() {
        bitField0_ = (bitField0_ & ~0x00000100);
        kernelTrackerMaxPending_ = 0;
        onChanged();
        return this;
      }

      private double internalFragmentationFraction_ ;
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       * @return The internalFragmentationFraction.
       */
      @java.lang.Override
      public double getInternalFragmentationFraction() {
        return internalFragmentationFraction_;
      }
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       * @param value The internalFragmentationFraction to set.
       * @return This builder for chaining.
       */
      public Builder setInternalFragmentationFraction(double value) {

        internalFragmentationFraction_ = value;
        bitField0_ |= 0x00000200;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearInternalFragmentationFraction() {
        bitField0_ = (bitField0_ & ~0x00000200);
        internalFragmentationFraction_ = 0D;
        onChanged();
        return this;
      }

      private boolean useCudaMallocAsync_ ;
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       * @return The useCudaMallocAsync.
       */
      @java.lang.Override
      public boolean getUseCudaMallocAsync() {
        return useCudaMallocAsync_;
      }
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       * @param value The useCudaMallocAsync to set.
       * @return This builder for chaining.
       */
      public Builder setUseCudaMallocAsync(boolean value) {

        useCudaMallocAsync_ = value;
        bitField0_ |= 0x00000400;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearUseCudaMallocAsync() {
        bitField0_ = (bitField0_ & ~0x00000400);
        useCudaMallocAsync_ = false;
        onChanged();
        return this;
      }

      private boolean disallowRetryOnAllocationFailure_ ;
      /**
       * <pre>
       * By default, BFCAllocator may sleep when it runs out of memory, in the
       * hopes that another thread will free up memory in the meantime.  Setting
       * this to true disables the sleep; instead we'll OOM immediately.
       * </pre>
       *
       * <code>bool disallow_retry_on_allocation_failure = 12;</code>
       * @return The disallowRetryOnAllocationFailure.
       */
      @java.lang.Override
      public boolean getDisallowRetryOnAllocationFailure() {
        return disallowRetryOnAllocationFailure_;
      }
      /**
       * <pre>
       * By default, BFCAllocator may sleep when it runs out of memory, in the
       * hopes that another thread will free up memory in the meantime.  Setting
       * this to true disables the sleep; instead we'll OOM immediately.
       * </pre>
       *
       * <code>bool disallow_retry_on_allocation_failure = 12;</code>
       * @param value The disallowRetryOnAllocationFailure to set.
       * @return This builder for chaining.
       */
      public Builder setDisallowRetryOnAllocationFailure(boolean value) {

        disallowRetryOnAllocationFailure_ = value;
        bitField0_ |= 0x00000800;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * By default, BFCAllocator may sleep when it runs out of memory, in the
       * hopes that another thread will free up memory in the meantime.  Setting
       * this to true disables the sleep; instead we'll OOM immediately.
       * </pre>
       *
       * <code>bool disallow_retry_on_allocation_failure = 12;</code>
       * @return This builder for chaining.
       */
      public Builder clearDisallowRetryOnAllocationFailure() {
        bitField0_ = (bitField0_ & ~0x00000800);
        disallowRetryOnAllocationFailure_ = false;
        onChanged();
        return this;
      }

      private float gpuHostMemLimitInMb_ ;
      /**
       * <pre>
       * Memory limit for "GPU host allocator", aka pinned memory allocator.  This
       * can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
       * </pre>
       *
       * <code>float gpu_host_mem_limit_in_mb = 13;</code>
       * @return The gpuHostMemLimitInMb.
       */
      @java.lang.Override
      public float getGpuHostMemLimitInMb() {
        return gpuHostMemLimitInMb_;
      }
      /**
       * <pre>
       * Memory limit for "GPU host allocator", aka pinned memory allocator.  This
       * can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
       * </pre>
       *
       * <code>float gpu_host_mem_limit_in_mb = 13;</code>
       * @param value The gpuHostMemLimitInMb to set.
       * @return This builder for chaining.
       */
      public Builder setGpuHostMemLimitInMb(float value) {

        gpuHostMemLimitInMb_ = value;
        bitField0_ |= 0x00001000;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Memory limit for "GPU host allocator", aka pinned memory allocator.  This
       * can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
       * </pre>
       *
       * <code>float gpu_host_mem_limit_in_mb = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearGpuHostMemLimitInMb() {
        bitField0_ = (bitField0_ & ~0x00001000);
        gpuHostMemLimitInMb_ = 0F;
        onChanged();
        return this;
      }

      private boolean gpuHostMemDisallowGrowth_ ;
      /**
       * <pre>
       * If true, then the host allocator allocates its max memory all upfront and
       * never grows.  This can be useful for latency-sensitive systems, because
       * growing the GPU host memory pool can be expensive.
       *
       * You probably only want to use this in combination with
       * gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
       * quite high.
       * </pre>
       *
       * <code>bool gpu_host_mem_disallow_growth = 14;</code>
       * @return The gpuHostMemDisallowGrowth.
       */
      @java.lang.Override
      public boolean getGpuHostMemDisallowGrowth() {
        return gpuHostMemDisallowGrowth_;
      }
      /**
       * <pre>
       * If true, then the host allocator allocates its max memory all upfront and
       * never grows.  This can be useful for latency-sensitive systems, because
       * growing the GPU host memory pool can be expensive.
       *
       * You probably only want to use this in combination with
       * gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
       * quite high.
       * </pre>
       *
       * <code>bool gpu_host_mem_disallow_growth = 14;</code>
       * @param value The gpuHostMemDisallowGrowth to set.
       * @return This builder for chaining.
       */
      public Builder setGpuHostMemDisallowGrowth(boolean value) {

        gpuHostMemDisallowGrowth_ = value;
        bitField0_ |= 0x00002000;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, then the host allocator allocates its max memory all upfront and
       * never grows.  This can be useful for latency-sensitive systems, because
       * growing the GPU host memory pool can be expensive.
       *
       * You probably only want to use this in combination with
       * gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
       * quite high.
       * </pre>
       *
       * <code>bool gpu_host_mem_disallow_growth = 14;</code>
       * @return This builder for chaining.
       */
      public Builder clearGpuHostMemDisallowGrowth() {
        bitField0_ = (bitField0_ & ~0x00002000);
        gpuHostMemDisallowGrowth_ = false;
        onChanged();
        return this;
      }

      private int gpuSystemMemorySizeInMb_ ;
      /**
       * <pre>
       * Memory limit for gpu system. This can also be set by
       * TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
       * gpu_system_memory_size_in_mb. With this, user can configure the gpu
       * system memory size for better resource estimation of multi-tenancy(one
       * gpu with multiple model) use case.
       * </pre>
       *
       * <code>int32 gpu_system_memory_size_in_mb = 16;</code>
       * @return The gpuSystemMemorySizeInMb.
       */
      @java.lang.Override
      public int getGpuSystemMemorySizeInMb() {
        return gpuSystemMemorySizeInMb_;
      }
      /**
       * <pre>
       * Memory limit for gpu system. This can also be set by
       * TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
       * gpu_system_memory_size_in_mb. With this, user can configure the gpu
       * system memory size for better resource estimation of multi-tenancy(one
       * gpu with multiple model) use case.
       * </pre>
       *
       * <code>int32 gpu_system_memory_size_in_mb = 16;</code>
       * @param value The gpuSystemMemorySizeInMb to set.
       * @return This builder for chaining.
       */
      public Builder setGpuSystemMemorySizeInMb(int value) {

        gpuSystemMemorySizeInMb_ = value;
        bitField0_ |= 0x00004000;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Memory limit for gpu system. This can also be set by
       * TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
       * gpu_system_memory_size_in_mb. With this, user can configure the gpu
       * system memory size for better resource estimation of multi-tenancy(one
       * gpu with multiple model) use case.
       * </pre>
       *
       * <code>int32 gpu_system_memory_size_in_mb = 16;</code>
       * @return This builder for chaining.
       */
      public Builder clearGpuSystemMemorySizeInMb() {
        bitField0_ = (bitField0_ & ~0x00004000);
        gpuSystemMemorySizeInMb_ = 0;
        onChanged();
        return this;
      }

      private boolean populatePjrtGpuClientCreationInfo_ ;
      /**
       * <pre>
       * If true, save information needed for created a PjRt GPU client for
       * creating a client with remote devices.
       * </pre>
       *
       * <code>bool populate_pjrt_gpu_client_creation_info = 17;</code>
       * @return The populatePjrtGpuClientCreationInfo.
       */
      @java.lang.Override
      public boolean getPopulatePjrtGpuClientCreationInfo() {
        return populatePjrtGpuClientCreationInfo_;
      }
      /**
       * <pre>
       * If true, save information needed for created a PjRt GPU client for
       * creating a client with remote devices.
       * </pre>
       *
       * <code>bool populate_pjrt_gpu_client_creation_info = 17;</code>
       * @param value The populatePjrtGpuClientCreationInfo to set.
       * @return This builder for chaining.
       */
      public Builder setPopulatePjrtGpuClientCreationInfo(boolean value) {

        populatePjrtGpuClientCreationInfo_ = value;
        bitField0_ |= 0x00008000;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, save information needed for created a PjRt GPU client for
       * creating a client with remote devices.
       * </pre>
       *
       * <code>bool populate_pjrt_gpu_client_creation_info = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearPopulatePjrtGpuClientCreationInfo() {
        bitField0_ = (bitField0_ & ~0x00008000);
        populatePjrtGpuClientCreationInfo_ = false;
        onChanged();
        return this;
      }

      private int nodeId_ ;
      /**
       * <pre>
       * node_id for use when creating a PjRt GPU client with remote devices,
       * which enumerates jobs*tasks from a ServerDef.
       * </pre>
       *
       * <code>int32 node_id = 18;</code>
       * @return The nodeId.
       */
      @java.lang.Override
      public int getNodeId() {
        return nodeId_;
      }
      /**
       * <pre>
       * node_id for use when creating a PjRt GPU client with remote devices,
       * which enumerates jobs*tasks from a ServerDef.
       * </pre>
       *
       * <code>int32 node_id = 18;</code>
       * @param value The nodeId to set.
       * @return This builder for chaining.
       */
      public Builder setNodeId(int value) {

        nodeId_ = value;
        bitField0_ |= 0x00010000;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * node_id for use when creating a PjRt GPU client with remote devices,
       * which enumerates jobs*tasks from a ServerDef.
       * </pre>
       *
       * <code>int32 node_id = 18;</code>
       * @return This builder for chaining.
       */
      public Builder clearNodeId() {
        bitField0_ = (bitField0_ & ~0x00010000);
        nodeId_ = 0;
        onChanged();
        return this;
      }

      private org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions streamMergeOptions_;
      private com.google.protobuf.SingleFieldBuilder<
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder> streamMergeOptionsBuilder_;
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       * @return Whether the streamMergeOptions field is set.
       */
      public boolean hasStreamMergeOptions() {
        return ((bitField0_ & 0x00020000) != 0);
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       * @return The streamMergeOptions.
       */
      public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions getStreamMergeOptions() {
        if (streamMergeOptionsBuilder_ == null) {
          return streamMergeOptions_ == null ? org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance() : streamMergeOptions_;
        } else {
          return streamMergeOptionsBuilder_.getMessage();
        }
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public Builder setStreamMergeOptions(org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions value) {
        if (streamMergeOptionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          streamMergeOptions_ = value;
        } else {
          streamMergeOptionsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00020000;
        onChanged();
        return this;
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public Builder setStreamMergeOptions(
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder builderForValue) {
        if (streamMergeOptionsBuilder_ == null) {
          streamMergeOptions_ = builderForValue.build();
        } else {
          streamMergeOptionsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00020000;
        onChanged();
        return this;
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public Builder mergeStreamMergeOptions(org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions value) {
        if (streamMergeOptionsBuilder_ == null) {
          if (((bitField0_ & 0x00020000) != 0) &&
            streamMergeOptions_ != null &&
            streamMergeOptions_ != org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance()) {
            getStreamMergeOptionsBuilder().mergeFrom(value);
          } else {
            streamMergeOptions_ = value;
          }
        } else {
          streamMergeOptionsBuilder_.mergeFrom(value);
        }
        if (streamMergeOptions_ != null) {
          bitField0_ |= 0x00020000;
          onChanged();
        }
        return this;
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public Builder clearStreamMergeOptions() {
        bitField0_ = (bitField0_ & ~0x00020000);
        streamMergeOptions_ = null;
        if (streamMergeOptionsBuilder_ != null) {
          streamMergeOptionsBuilder_.dispose();
          streamMergeOptionsBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder getStreamMergeOptionsBuilder() {
        bitField0_ |= 0x00020000;
        onChanged();
        return getStreamMergeOptionsFieldBuilder().getBuilder();
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      public org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder getStreamMergeOptionsOrBuilder() {
        if (streamMergeOptionsBuilder_ != null) {
          return streamMergeOptionsBuilder_.getMessageOrBuilder();
        } else {
          return streamMergeOptions_ == null ?
              org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.getDefaultInstance() : streamMergeOptions_;
        }
      }
      /**
       * <code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder> 
          getStreamMergeOptionsFieldBuilder() {
        if (streamMergeOptionsBuilder_ == null) {
          streamMergeOptionsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptions.Builder, org.tensorflow.proto.GPUOptions.Experimental.StreamMergeOptionsOrBuilder>(
                  getStreamMergeOptions(),
                  getParentForChildren(),
                  isClean());
          streamMergeOptions_ = null;
        }
        return streamMergeOptionsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions.Experimental)
    }

    // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental)
    private static final org.tensorflow.proto.GPUOptions.Experimental DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.tensorflow.proto.GPUOptions.Experimental();
    }

    public static org.tensorflow.proto.GPUOptions.Experimental getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Experimental>
        PARSER = new com.google.protobuf.AbstractParser<Experimental>() {
      @java.lang.Override
      public Experimental parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Experimental> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Experimental> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.tensorflow.proto.GPUOptions.Experimental getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private int bitField0_;
  public static final int PER_PROCESS_GPU_MEMORY_FRACTION_FIELD_NUMBER = 1;
  private double perProcessGpuMemoryFraction_ = 0D;
  /**
   * <pre>
   * Fraction of the total GPU memory to allocate for each process.
   * 1 means to allocate all of the GPU memory, 0.5 means the process
   * allocates up to ~50% of the total GPU memory.
   *
   * GPU memory is pre-allocated unless the allow_growth option is enabled.
   *
   * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
   * the amount of memory available on the GPU device by using host memory as a
   * swap space. Accessing memory not available on the device will be
   * significantly slower as that would require memory transfer between the host
   * and the device. Options to reduce the memory requirement should be
   * considered before enabling this option as this may come with a negative
   * performance impact. Oversubscription using the unified memory requires
   * Pascal class or newer GPUs and it is currently only supported on the Linux
   * operating system. See
   * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
   * for the detailed requirements.
   * </pre>
   *
   * <code>double per_process_gpu_memory_fraction = 1;</code>
   * @return The perProcessGpuMemoryFraction.
   */
  @java.lang.Override
  public double getPerProcessGpuMemoryFraction() {
    return perProcessGpuMemoryFraction_;
  }

  public static final int ALLOW_GROWTH_FIELD_NUMBER = 4;
  private boolean allowGrowth_ = false;
  /**
   * <pre>
   * If true, the allocator does not pre-allocate the entire specified
   * GPU memory region, instead starting small and growing as needed.
   * </pre>
   *
   * <code>bool allow_growth = 4;</code>
   * @return The allowGrowth.
   */
  @java.lang.Override
  public boolean getAllowGrowth() {
    return allowGrowth_;
  }

  public static final int ALLOCATOR_TYPE_FIELD_NUMBER = 2;
  @SuppressWarnings("serial")
  private volatile java.lang.Object allocatorType_ = "";
  /**
   * <pre>
   * The type of GPU allocation strategy to use.
   *
   * Allowed values:
   * "": The empty string (default) uses a system-chosen default
   * which may change over time.
   *
   * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
   * version of dlmalloc.
   * </pre>
   *
   * <code>string allocator_type = 2;</code>
   * @return The allocatorType.
   */
  @java.lang.Override
  public java.lang.String getAllocatorType() {
    java.lang.Object ref = allocatorType_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      allocatorType_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * The type of GPU allocation strategy to use.
   *
   * Allowed values:
   * "": The empty string (default) uses a system-chosen default
   * which may change over time.
   *
   * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
   * version of dlmalloc.
   * </pre>
   *
   * <code>string allocator_type = 2;</code>
   * @return The bytes for allocatorType.
   */
  @java.lang.Override
  public com.google.protobuf.ByteString
      getAllocatorTypeBytes() {
    java.lang.Object ref = allocatorType_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (java.lang.String) ref);
      allocatorType_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int DEFERRED_DELETION_BYTES_FIELD_NUMBER = 3;
  private long deferredDeletionBytes_ = 0L;
  /**
   * <pre>
   * Delay deletion of up to this many bytes to reduce the number of
   * interactions with gpu driver code.  If 0, the system chooses
   * a reasonable default (several MBs).
   * </pre>
   *
   * <code>int64 deferred_deletion_bytes = 3;</code>
   * @return The deferredDeletionBytes.
   */
  @java.lang.Override
  public long getDeferredDeletionBytes() {
    return deferredDeletionBytes_;
  }

  public static final int VISIBLE_DEVICE_LIST_FIELD_NUMBER = 5;
  @SuppressWarnings("serial")
  private volatile java.lang.Object visibleDeviceList_ = "";
  /**
   * <pre>
   * A comma-separated list of GPU ids that determines the 'visible'
   * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
   * can see 8 GPU devices in the process, and one wanted to map
   * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
   * then one would specify this field as "5,3".  This field is similar in
   * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
   * it applies to the visible GPU devices in the process.
   *
   * NOTE:
   * 1. The GPU driver provides the process with the visible GPUs
   * in an order which is not guaranteed to have any correlation to
   * the *physical* GPU id in the machine.  This field is used for
   * remapping "visible" to "virtual", which means this operates only
   * after the process starts.  Users are required to use vendor
   * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
   * physical to visible device mapping prior to invoking TensorFlow.
   * 2. In the code, the ids in this list are also called "platform GPU id"s,
   * and the 'virtual' ids of GPU devices (i.e. the ids in the device
   * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
   * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
   * for more information.
   * 3. The visible_device_list is also used for PluggableDevice. And
   * different types of PluggableDevices share this field. In that case,
   * the pluggable_device_type is used to distinguish them, making the
   * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
   * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
   * </pre>
   *
   * <code>string visible_device_list = 5;</code>
   * @return The visibleDeviceList.
   */
  @java.lang.Override
  public java.lang.String getVisibleDeviceList() {
    java.lang.Object ref = visibleDeviceList_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      visibleDeviceList_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * A comma-separated list of GPU ids that determines the 'visible'
   * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
   * can see 8 GPU devices in the process, and one wanted to map
   * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
   * then one would specify this field as "5,3".  This field is similar in
   * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
   * it applies to the visible GPU devices in the process.
   *
   * NOTE:
   * 1. The GPU driver provides the process with the visible GPUs
   * in an order which is not guaranteed to have any correlation to
   * the *physical* GPU id in the machine.  This field is used for
   * remapping "visible" to "virtual", which means this operates only
   * after the process starts.  Users are required to use vendor
   * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
   * physical to visible device mapping prior to invoking TensorFlow.
   * 2. In the code, the ids in this list are also called "platform GPU id"s,
   * and the 'virtual' ids of GPU devices (i.e. the ids in the device
   * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
   * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
   * for more information.
   * 3. The visible_device_list is also used for PluggableDevice. And
   * different types of PluggableDevices share this field. In that case,
   * the pluggable_device_type is used to distinguish them, making the
   * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
   * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
   * </pre>
   *
   * <code>string visible_device_list = 5;</code>
   * @return The bytes for visibleDeviceList.
   */
  @java.lang.Override
  public com.google.protobuf.ByteString
      getVisibleDeviceListBytes() {
    java.lang.Object ref = visibleDeviceList_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (java.lang.String) ref);
      visibleDeviceList_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int POLLING_ACTIVE_DELAY_USECS_FIELD_NUMBER = 6;
  private int pollingActiveDelayUsecs_ = 0;
  /**
   * <pre>
   * In the event polling loop sleep this many microseconds between
   * PollEvents calls, when the queue is not empty.  If value is not
   * set or set to 0, gets set to a non-zero default.
   * </pre>
   *
   * <code>int32 polling_active_delay_usecs = 6;</code>
   * @return The pollingActiveDelayUsecs.
   */
  @java.lang.Override
  public int getPollingActiveDelayUsecs() {
    return pollingActiveDelayUsecs_;
  }

  public static final int POLLING_INACTIVE_DELAY_MSECS_FIELD_NUMBER = 7;
  private int pollingInactiveDelayMsecs_ = 0;
  /**
   * <pre>
   * This field is deprecated and ignored.
   * </pre>
   *
   * <code>int32 polling_inactive_delay_msecs = 7;</code>
   * @return The pollingInactiveDelayMsecs.
   */
  @java.lang.Override
  public int getPollingInactiveDelayMsecs() {
    return pollingInactiveDelayMsecs_;
  }

  public static final int FORCE_GPU_COMPATIBLE_FIELD_NUMBER = 8;
  private boolean forceGpuCompatible_ = false;
  /**
   * <pre>
   * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
   * enabling this option forces all CPU tensors to be allocated with Cuda
   * pinned memory. Normally, TensorFlow will infer which tensors should be
   * allocated as the pinned memory. But in case where the inference is
   * incomplete, this option can significantly speed up the cross-device memory
   * copy performance as long as it fits the memory.
   * Note that this option is not something that should be
   * enabled by default for unknown or very large models, since all Cuda pinned
   * memory is unpageable, having too much pinned memory might negatively impact
   * the overall host system performance.
   * </pre>
   *
   * <code>bool force_gpu_compatible = 8;</code>
   * @return The forceGpuCompatible.
   */
  @java.lang.Override
  public boolean getForceGpuCompatible() {
    return forceGpuCompatible_;
  }

  public static final int EXPERIMENTAL_FIELD_NUMBER = 9;
  private org.tensorflow.proto.GPUOptions.Experimental experimental_;
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/versions.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   * @return Whether the experimental field is set.
   */
  @java.lang.Override
  public boolean hasExperimental() {
    return ((bitField0_ & 0x00000001) != 0);
  }
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/versions.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   * @return The experimental.
   */
  @java.lang.Override
  public org.tensorflow.proto.GPUOptions.Experimental getExperimental() {
    return experimental_ == null ? org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance() : experimental_;
  }
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/versions.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   */
  @java.lang.Override
  public org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder getExperimentalOrBuilder() {
    return experimental_ == null ? org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance() : experimental_;
  }

  private byte memoizedIsInitialized = -1;
  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (java.lang.Double.doubleToRawLongBits(perProcessGpuMemoryFraction_) != 0) {
      output.writeDouble(1, perProcessGpuMemoryFraction_);
    }
    if (!com.google.protobuf.GeneratedMessage.isStringEmpty(allocatorType_)) {
      com.google.protobuf.GeneratedMessage.writeString(output, 2, allocatorType_);
    }
    if (deferredDeletionBytes_ != 0L) {
      output.writeInt64(3, deferredDeletionBytes_);
    }
    if (allowGrowth_ != false) {
      output.writeBool(4, allowGrowth_);
    }
    if (!com.google.protobuf.GeneratedMessage.isStringEmpty(visibleDeviceList_)) {
      com.google.protobuf.GeneratedMessage.writeString(output, 5, visibleDeviceList_);
    }
    if (pollingActiveDelayUsecs_ != 0) {
      output.writeInt32(6, pollingActiveDelayUsecs_);
    }
    if (pollingInactiveDelayMsecs_ != 0) {
      output.writeInt32(7, pollingInactiveDelayMsecs_);
    }
    if (forceGpuCompatible_ != false) {
      output.writeBool(8, forceGpuCompatible_);
    }
    if (((bitField0_ & 0x00000001) != 0)) {
      output.writeMessage(9, getExperimental());
    }
    getUnknownFields().writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    if (java.lang.Double.doubleToRawLongBits(perProcessGpuMemoryFraction_) != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeDoubleSize(1, perProcessGpuMemoryFraction_);
    }
    if (!com.google.protobuf.GeneratedMessage.isStringEmpty(allocatorType_)) {
      size += com.google.protobuf.GeneratedMessage.computeStringSize(2, allocatorType_);
    }
    if (deferredDeletionBytes_ != 0L) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt64Size(3, deferredDeletionBytes_);
    }
    if (allowGrowth_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(4, allowGrowth_);
    }
    if (!com.google.protobuf.GeneratedMessage.isStringEmpty(visibleDeviceList_)) {
      size += com.google.protobuf.GeneratedMessage.computeStringSize(5, visibleDeviceList_);
    }
    if (pollingActiveDelayUsecs_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(6, pollingActiveDelayUsecs_);
    }
    if (pollingInactiveDelayMsecs_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(7, pollingInactiveDelayMsecs_);
    }
    if (forceGpuCompatible_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(8, forceGpuCompatible_);
    }
    if (((bitField0_ & 0x00000001) != 0)) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(9, getExperimental());
    }
    size += getUnknownFields().getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
     return true;
    }
    if (!(obj instanceof org.tensorflow.proto.GPUOptions)) {
      return super.equals(obj);
    }
    org.tensorflow.proto.GPUOptions other = (org.tensorflow.proto.GPUOptions) obj;

    if (java.lang.Double.doubleToLongBits(getPerProcessGpuMemoryFraction())
        != java.lang.Double.doubleToLongBits(
            other.getPerProcessGpuMemoryFraction())) return false;
    if (getAllowGrowth()
        != other.getAllowGrowth()) return false;
    if (!getAllocatorType()
        .equals(other.getAllocatorType())) return false;
    if (getDeferredDeletionBytes()
        != other.getDeferredDeletionBytes()) return false;
    if (!getVisibleDeviceList()
        .equals(other.getVisibleDeviceList())) return false;
    if (getPollingActiveDelayUsecs()
        != other.getPollingActiveDelayUsecs()) return false;
    if (getPollingInactiveDelayMsecs()
        != other.getPollingInactiveDelayMsecs()) return false;
    if (getForceGpuCompatible()
        != other.getForceGpuCompatible()) return false;
    if (hasExperimental() != other.hasExperimental()) return false;
    if (hasExperimental()) {
      if (!getExperimental()
          .equals(other.getExperimental())) return false;
    }
    if (!getUnknownFields().equals(other.getUnknownFields())) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    hash = (37 * hash) + PER_PROCESS_GPU_MEMORY_FRACTION_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
        java.lang.Double.doubleToLongBits(getPerProcessGpuMemoryFraction()));
    hash = (37 * hash) + ALLOW_GROWTH_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getAllowGrowth());
    hash = (37 * hash) + ALLOCATOR_TYPE_FIELD_NUMBER;
    hash = (53 * hash) + getAllocatorType().hashCode();
    hash = (37 * hash) + DEFERRED_DELETION_BYTES_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
        getDeferredDeletionBytes());
    hash = (37 * hash) + VISIBLE_DEVICE_LIST_FIELD_NUMBER;
    hash = (53 * hash) + getVisibleDeviceList().hashCode();
    hash = (37 * hash) + POLLING_ACTIVE_DELAY_USECS_FIELD_NUMBER;
    hash = (53 * hash) + getPollingActiveDelayUsecs();
    hash = (37 * hash) + POLLING_INACTIVE_DELAY_MSECS_FIELD_NUMBER;
    hash = (53 * hash) + getPollingInactiveDelayMsecs();
    hash = (37 * hash) + FORCE_GPU_COMPATIBLE_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getForceGpuCompatible());
    if (hasExperimental()) {
      hash = (37 * hash) + EXPERIMENTAL_FIELD_NUMBER;
      hash = (53 * hash) + getExperimental().hashCode();
    }
    hash = (29 * hash) + getUnknownFields().hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static org.tensorflow.proto.GPUOptions parseFrom(
      java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      java.nio.ByteBuffer data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  public static org.tensorflow.proto.GPUOptions parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseDelimitedWithIOException(PARSER, input);
  }

  public static org.tensorflow.proto.GPUOptions parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.GPUOptions parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() { return newBuilder(); }
  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(org.tensorflow.proto.GPUOptions prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }
  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE
        ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(
      com.google.protobuf.GeneratedMessage.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   * Protobuf type {@code tensorflow.GPUOptions}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessage.Builder<Builder> implements
      // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions)
      org.tensorflow.proto.GPUOptionsOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.GPUOptions.class, org.tensorflow.proto.GPUOptions.Builder.class);
    }

    // Construct using org.tensorflow.proto.GPUOptions.newBuilder()
    private Builder() {
      maybeForceBuilderInitialization();
    }

    private Builder(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      super(parent);
      maybeForceBuilderInitialization();
    }
    private void maybeForceBuilderInitialization() {
      if (com.google.protobuf.GeneratedMessage
              .alwaysUseFieldBuilders) {
        getExperimentalFieldBuilder();
      }
    }
    @java.lang.Override
    public Builder clear() {
      super.clear();
      bitField0_ = 0;
      perProcessGpuMemoryFraction_ = 0D;
      allowGrowth_ = false;
      allocatorType_ = "";
      deferredDeletionBytes_ = 0L;
      visibleDeviceList_ = "";
      pollingActiveDelayUsecs_ = 0;
      pollingInactiveDelayMsecs_ = 0;
      forceGpuCompatible_ = false;
      experimental_ = null;
      if (experimentalBuilder_ != null) {
        experimentalBuilder_.dispose();
        experimentalBuilder_ = null;
      }
      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor
        getDescriptorForType() {
      return org.tensorflow.proto.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
    }

    @java.lang.Override
    public org.tensorflow.proto.GPUOptions getDefaultInstanceForType() {
      return org.tensorflow.proto.GPUOptions.getDefaultInstance();
    }

    @java.lang.Override
    public org.tensorflow.proto.GPUOptions build() {
      org.tensorflow.proto.GPUOptions result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public org.tensorflow.proto.GPUOptions buildPartial() {
      org.tensorflow.proto.GPUOptions result = new org.tensorflow.proto.GPUOptions(this);
      if (bitField0_ != 0) { buildPartial0(result); }
      onBuilt();
      return result;
    }

    private void buildPartial0(org.tensorflow.proto.GPUOptions result) {
      int from_bitField0_ = bitField0_;
      if (((from_bitField0_ & 0x00000001) != 0)) {
        result.perProcessGpuMemoryFraction_ = perProcessGpuMemoryFraction_;
      }
      if (((from_bitField0_ & 0x00000002) != 0)) {
        result.allowGrowth_ = allowGrowth_;
      }
      if (((from_bitField0_ & 0x00000004) != 0)) {
        result.allocatorType_ = allocatorType_;
      }
      if (((from_bitField0_ & 0x00000008) != 0)) {
        result.deferredDeletionBytes_ = deferredDeletionBytes_;
      }
      if (((from_bitField0_ & 0x00000010) != 0)) {
        result.visibleDeviceList_ = visibleDeviceList_;
      }
      if (((from_bitField0_ & 0x00000020) != 0)) {
        result.pollingActiveDelayUsecs_ = pollingActiveDelayUsecs_;
      }
      if (((from_bitField0_ & 0x00000040) != 0)) {
        result.pollingInactiveDelayMsecs_ = pollingInactiveDelayMsecs_;
      }
      if (((from_bitField0_ & 0x00000080) != 0)) {
        result.forceGpuCompatible_ = forceGpuCompatible_;
      }
      int to_bitField0_ = 0;
      if (((from_bitField0_ & 0x00000100) != 0)) {
        result.experimental_ = experimentalBuilder_ == null
            ? experimental_
            : experimentalBuilder_.build();
        to_bitField0_ |= 0x00000001;
      }
      result.bitField0_ |= to_bitField0_;
    }

    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof org.tensorflow.proto.GPUOptions) {
        return mergeFrom((org.tensorflow.proto.GPUOptions)other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(org.tensorflow.proto.GPUOptions other) {
      if (other == org.tensorflow.proto.GPUOptions.getDefaultInstance()) return this;
      if (other.getPerProcessGpuMemoryFraction() != 0D) {
        setPerProcessGpuMemoryFraction(other.getPerProcessGpuMemoryFraction());
      }
      if (other.getAllowGrowth() != false) {
        setAllowGrowth(other.getAllowGrowth());
      }
      if (!other.getAllocatorType().isEmpty()) {
        allocatorType_ = other.allocatorType_;
        bitField0_ |= 0x00000004;
        onChanged();
      }
      if (other.getDeferredDeletionBytes() != 0L) {
        setDeferredDeletionBytes(other.getDeferredDeletionBytes());
      }
      if (!other.getVisibleDeviceList().isEmpty()) {
        visibleDeviceList_ = other.visibleDeviceList_;
        bitField0_ |= 0x00000010;
        onChanged();
      }
      if (other.getPollingActiveDelayUsecs() != 0) {
        setPollingActiveDelayUsecs(other.getPollingActiveDelayUsecs());
      }
      if (other.getPollingInactiveDelayMsecs() != 0) {
        setPollingInactiveDelayMsecs(other.getPollingInactiveDelayMsecs());
      }
      if (other.getForceGpuCompatible() != false) {
        setForceGpuCompatible(other.getForceGpuCompatible());
      }
      if (other.hasExperimental()) {
        mergeExperimental(other.getExperimental());
      }
      this.mergeUnknownFields(other.getUnknownFields());
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {
              perProcessGpuMemoryFraction_ = input.readDouble();
              bitField0_ |= 0x00000001;
              break;
            } // case 9
            case 18: {
              allocatorType_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              break;
            } // case 18
            case 24: {
              deferredDeletionBytes_ = input.readInt64();
              bitField0_ |= 0x00000008;
              break;
            } // case 24
            case 32: {
              allowGrowth_ = input.readBool();
              bitField0_ |= 0x00000002;
              break;
            } // case 32
            case 42: {
              visibleDeviceList_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000010;
              break;
            } // case 42
            case 48: {
              pollingActiveDelayUsecs_ = input.readInt32();
              bitField0_ |= 0x00000020;
              break;
            } // case 48
            case 56: {
              pollingInactiveDelayMsecs_ = input.readInt32();
              bitField0_ |= 0x00000040;
              break;
            } // case 56
            case 64: {
              forceGpuCompatible_ = input.readBool();
              bitField0_ |= 0x00000080;
              break;
            } // case 64
            case 74: {
              input.readMessage(
                  getExperimentalFieldBuilder().getBuilder(),
                  extensionRegistry);
              bitField0_ |= 0x00000100;
              break;
            } // case 74
            default: {
              if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                done = true; // was an endgroup tag
              }
              break;
            } // default:
          } // switch (tag)
        } // while (!done)
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.unwrapIOException();
      } finally {
        onChanged();
      } // finally
      return this;
    }
    private int bitField0_;

    private double perProcessGpuMemoryFraction_ ;
    /**
     * <pre>
     * Fraction of the total GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the total GPU memory.
     *
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     *
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     * @return The perProcessGpuMemoryFraction.
     */
    @java.lang.Override
    public double getPerProcessGpuMemoryFraction() {
      return perProcessGpuMemoryFraction_;
    }
    /**
     * <pre>
     * Fraction of the total GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the total GPU memory.
     *
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     *
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     * @param value The perProcessGpuMemoryFraction to set.
     * @return This builder for chaining.
     */
    public Builder setPerProcessGpuMemoryFraction(double value) {

      perProcessGpuMemoryFraction_ = value;
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Fraction of the total GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the total GPU memory.
     *
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     *
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     * @return This builder for chaining.
     */
    public Builder clearPerProcessGpuMemoryFraction() {
      bitField0_ = (bitField0_ & ~0x00000001);
      perProcessGpuMemoryFraction_ = 0D;
      onChanged();
      return this;
    }

    private boolean allowGrowth_ ;
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     * @return The allowGrowth.
     */
    @java.lang.Override
    public boolean getAllowGrowth() {
      return allowGrowth_;
    }
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     * @param value The allowGrowth to set.
     * @return This builder for chaining.
     */
    public Builder setAllowGrowth(boolean value) {

      allowGrowth_ = value;
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     * @return This builder for chaining.
     */
    public Builder clearAllowGrowth() {
      bitField0_ = (bitField0_ & ~0x00000002);
      allowGrowth_ = false;
      onChanged();
      return this;
    }

    private java.lang.Object allocatorType_ = "";
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     *
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     * which may change over time.
     *
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     * version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     * @return The allocatorType.
     */
    public java.lang.String getAllocatorType() {
      java.lang.Object ref = allocatorType_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        allocatorType_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     *
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     * which may change over time.
     *
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     * version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     * @return The bytes for allocatorType.
     */
    public com.google.protobuf.ByteString
        getAllocatorTypeBytes() {
      java.lang.Object ref = allocatorType_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        allocatorType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     *
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     * which may change over time.
     *
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     * version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     * @param value The allocatorType to set.
     * @return This builder for chaining.
     */
    public Builder setAllocatorType(
        java.lang.String value) {
      if (value == null) { throw new NullPointerException(); }
      allocatorType_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     *
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     * which may change over time.
     *
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     * version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     * @return This builder for chaining.
     */
    public Builder clearAllocatorType() {
      allocatorType_ = getDefaultInstance().getAllocatorType();
      bitField0_ = (bitField0_ & ~0x00000004);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     *
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     * which may change over time.
     *
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     * version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     * @param value The bytes for allocatorType to set.
     * @return This builder for chaining.
     */
    public Builder setAllocatorTypeBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      allocatorType_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }

    private long deferredDeletionBytes_ ;
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     * @return The deferredDeletionBytes.
     */
    @java.lang.Override
    public long getDeferredDeletionBytes() {
      return deferredDeletionBytes_;
    }
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     * @param value The deferredDeletionBytes to set.
     * @return This builder for chaining.
     */
    public Builder setDeferredDeletionBytes(long value) {

      deferredDeletionBytes_ = value;
      bitField0_ |= 0x00000008;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     * @return This builder for chaining.
     */
    public Builder clearDeferredDeletionBytes() {
      bitField0_ = (bitField0_ & ~0x00000008);
      deferredDeletionBytes_ = 0L;
      onChanged();
      return this;
    }

    private java.lang.Object visibleDeviceList_ = "";
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     *
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     * in an order which is not guaranteed to have any correlation to
     * the *physical* GPU id in the machine.  This field is used for
     * remapping "visible" to "virtual", which means this operates only
     * after the process starts.  Users are required to use vendor
     * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     * physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     * and the 'virtual' ids of GPU devices (i.e. the ids in the device
     * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     * for more information.
     * 3. The visible_device_list is also used for PluggableDevice. And
     * different types of PluggableDevices share this field. In that case,
     * the pluggable_device_type is used to distinguish them, making the
     * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
     * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     * @return The visibleDeviceList.
     */
    public java.lang.String getVisibleDeviceList() {
      java.lang.Object ref = visibleDeviceList_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        visibleDeviceList_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     *
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     * in an order which is not guaranteed to have any correlation to
     * the *physical* GPU id in the machine.  This field is used for
     * remapping "visible" to "virtual", which means this operates only
     * after the process starts.  Users are required to use vendor
     * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     * physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     * and the 'virtual' ids of GPU devices (i.e. the ids in the device
     * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     * for more information.
     * 3. The visible_device_list is also used for PluggableDevice. And
     * different types of PluggableDevices share this field. In that case,
     * the pluggable_device_type is used to distinguish them, making the
     * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
     * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     * @return The bytes for visibleDeviceList.
     */
    public com.google.protobuf.ByteString
        getVisibleDeviceListBytes() {
      java.lang.Object ref = visibleDeviceList_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        visibleDeviceList_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     *
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     * in an order which is not guaranteed to have any correlation to
     * the *physical* GPU id in the machine.  This field is used for
     * remapping "visible" to "virtual", which means this operates only
     * after the process starts.  Users are required to use vendor
     * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     * physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     * and the 'virtual' ids of GPU devices (i.e. the ids in the device
     * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     * for more information.
     * 3. The visible_device_list is also used for PluggableDevice. And
     * different types of PluggableDevices share this field. In that case,
     * the pluggable_device_type is used to distinguish them, making the
     * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
     * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     * @param value The visibleDeviceList to set.
     * @return This builder for chaining.
     */
    public Builder setVisibleDeviceList(
        java.lang.String value) {
      if (value == null) { throw new NullPointerException(); }
      visibleDeviceList_ = value;
      bitField0_ |= 0x00000010;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     *
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     * in an order which is not guaranteed to have any correlation to
     * the *physical* GPU id in the machine.  This field is used for
     * remapping "visible" to "virtual", which means this operates only
     * after the process starts.  Users are required to use vendor
     * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     * physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     * and the 'virtual' ids of GPU devices (i.e. the ids in the device
     * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     * for more information.
     * 3. The visible_device_list is also used for PluggableDevice. And
     * different types of PluggableDevices share this field. In that case,
     * the pluggable_device_type is used to distinguish them, making the
     * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
     * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     * @return This builder for chaining.
     */
    public Builder clearVisibleDeviceList() {
      visibleDeviceList_ = getDefaultInstance().getVisibleDeviceList();
      bitField0_ = (bitField0_ & ~0x00000010);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     *
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     * in an order which is not guaranteed to have any correlation to
     * the *physical* GPU id in the machine.  This field is used for
     * remapping "visible" to "virtual", which means this operates only
     * after the process starts.  Users are required to use vendor
     * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     * physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     * and the 'virtual' ids of GPU devices (i.e. the ids in the device
     * name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     * refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     * for more information.
     * 3. The visible_device_list is also used for PluggableDevice. And
     * different types of PluggableDevices share this field. In that case,
     * the pluggable_device_type is used to distinguish them, making the
     * visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
     * e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     * @param value The bytes for visibleDeviceList to set.
     * @return This builder for chaining.
     */
    public Builder setVisibleDeviceListBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      visibleDeviceList_ = value;
      bitField0_ |= 0x00000010;
      onChanged();
      return this;
    }

    private int pollingActiveDelayUsecs_ ;
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     * @return The pollingActiveDelayUsecs.
     */
    @java.lang.Override
    public int getPollingActiveDelayUsecs() {
      return pollingActiveDelayUsecs_;
    }
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     * @param value The pollingActiveDelayUsecs to set.
     * @return This builder for chaining.
     */
    public Builder setPollingActiveDelayUsecs(int value) {

      pollingActiveDelayUsecs_ = value;
      bitField0_ |= 0x00000020;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     * @return This builder for chaining.
     */
    public Builder clearPollingActiveDelayUsecs() {
      bitField0_ = (bitField0_ & ~0x00000020);
      pollingActiveDelayUsecs_ = 0;
      onChanged();
      return this;
    }

    private int pollingInactiveDelayMsecs_ ;
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     * @return The pollingInactiveDelayMsecs.
     */
    @java.lang.Override
    public int getPollingInactiveDelayMsecs() {
      return pollingInactiveDelayMsecs_;
    }
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     * @param value The pollingInactiveDelayMsecs to set.
     * @return This builder for chaining.
     */
    public Builder setPollingInactiveDelayMsecs(int value) {

      pollingInactiveDelayMsecs_ = value;
      bitField0_ |= 0x00000040;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     * @return This builder for chaining.
     */
    public Builder clearPollingInactiveDelayMsecs() {
      bitField0_ = (bitField0_ & ~0x00000040);
      pollingInactiveDelayMsecs_ = 0;
      onChanged();
      return this;
    }

    private boolean forceGpuCompatible_ ;
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     * @return The forceGpuCompatible.
     */
    @java.lang.Override
    public boolean getForceGpuCompatible() {
      return forceGpuCompatible_;
    }
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     * @param value The forceGpuCompatible to set.
     * @return This builder for chaining.
     */
    public Builder setForceGpuCompatible(boolean value) {

      forceGpuCompatible_ = value;
      bitField0_ |= 0x00000080;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     * @return This builder for chaining.
     */
    public Builder clearForceGpuCompatible() {
      bitField0_ = (bitField0_ & ~0x00000080);
      forceGpuCompatible_ = false;
      onChanged();
      return this;
    }

    private org.tensorflow.proto.GPUOptions.Experimental experimental_;
    private com.google.protobuf.SingleFieldBuilder<
        org.tensorflow.proto.GPUOptions.Experimental, org.tensorflow.proto.GPUOptions.Experimental.Builder, org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder> experimentalBuilder_;
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     * @return Whether the experimental field is set.
     */
    public boolean hasExperimental() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     * @return The experimental.
     */
    public org.tensorflow.proto.GPUOptions.Experimental getExperimental() {
      if (experimentalBuilder_ == null) {
        return experimental_ == null ? org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance() : experimental_;
      } else {
        return experimentalBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder setExperimental(org.tensorflow.proto.GPUOptions.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        experimental_ = value;
      } else {
        experimentalBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00000100;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder setExperimental(
        org.tensorflow.proto.GPUOptions.Experimental.Builder builderForValue) {
      if (experimentalBuilder_ == null) {
        experimental_ = builderForValue.build();
      } else {
        experimentalBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00000100;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder mergeExperimental(org.tensorflow.proto.GPUOptions.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (((bitField0_ & 0x00000100) != 0) &&
          experimental_ != null &&
          experimental_ != org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance()) {
          getExperimentalBuilder().mergeFrom(value);
        } else {
          experimental_ = value;
        }
      } else {
        experimentalBuilder_.mergeFrom(value);
      }
      if (experimental_ != null) {
        bitField0_ |= 0x00000100;
        onChanged();
      }
      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder clearExperimental() {
      bitField0_ = (bitField0_ & ~0x00000100);
      experimental_ = null;
      if (experimentalBuilder_ != null) {
        experimentalBuilder_.dispose();
        experimentalBuilder_ = null;
      }
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public org.tensorflow.proto.GPUOptions.Experimental.Builder getExperimentalBuilder() {
      bitField0_ |= 0x00000100;
      onChanged();
      return getExperimentalFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder getExperimentalOrBuilder() {
      if (experimentalBuilder_ != null) {
        return experimentalBuilder_.getMessageOrBuilder();
      } else {
        return experimental_ == null ?
            org.tensorflow.proto.GPUOptions.Experimental.getDefaultInstance() : experimental_;
      }
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/versions.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    private com.google.protobuf.SingleFieldBuilder<
        org.tensorflow.proto.GPUOptions.Experimental, org.tensorflow.proto.GPUOptions.Experimental.Builder, org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder> 
        getExperimentalFieldBuilder() {
      if (experimentalBuilder_ == null) {
        experimentalBuilder_ = new com.google.protobuf.SingleFieldBuilder<
            org.tensorflow.proto.GPUOptions.Experimental, org.tensorflow.proto.GPUOptions.Experimental.Builder, org.tensorflow.proto.GPUOptions.ExperimentalOrBuilder>(
                getExperimental(),
                getParentForChildren(),
                isClean());
        experimental_ = null;
      }
      return experimentalBuilder_;
    }

    // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions)
  }

  // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions)
  private static final org.tensorflow.proto.GPUOptions DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new org.tensorflow.proto.GPUOptions();
  }

  public static org.tensorflow.proto.GPUOptions getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<GPUOptions>
      PARSER = new com.google.protobuf.AbstractParser<GPUOptions>() {
    @java.lang.Override
    public GPUOptions parsePartialFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      Builder builder = newBuilder();
      try {
        builder.mergeFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(builder.buildPartial());
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(e)
            .setUnfinishedMessage(builder.buildPartial());
      }
      return builder.buildPartial();
    }
  };

  public static com.google.protobuf.Parser<GPUOptions> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<GPUOptions> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public org.tensorflow.proto.GPUOptions getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

}

